<?xml version="1.0" ?>
<!DOCTYPE book PUBLIC "-//KDE//DTD DocBook XML V4.2-Based Variant V1.1//EN" "dtd/kdex.dtd" [
  <!-- Define an entity for your application if it is not part of KDE
       CVS -->
  <!ENTITY kmyapplication "<application>simon</application>">
  <!ENTITY kappname "&kmyapplication;">
  <!ENTITY package "kde-module"><!-- kdebase, kdeadmin, etc.  Leave
                                     this unchanged if your
                                     application is not maintained in KDE CVS -->
  <!ENTITY % addindex "IGNORE">
  <!ENTITY % English "INCLUDE"> 

]>


<book lang="&language;">

<bookinfo>
<title>The &kmyapplication; Handbook</title>

<copyright>
<year>2008-2010</year>
<holder>Peter Grasch</holder>
</copyright>

<date>2010-07-25</date>
<releaseinfo>0.3</releaseinfo>

<legalnotice>&FDLNotice;</legalnotice>

<authorgroup>
<author>
<personname>
<firstname>Peter</firstname>
<othername>H.</othername>
<surname>Grasch</surname>
</personname>
<email>grasch@simon-listens.org</email>
</author>
</authorgroup>

<abstract>
<para>
&kmyapplication; is an open source speech recognition solution.
</para>
</abstract>

<keywordset>
<keyword>KDE</keyword>
<keyword>kdeutils</keyword>
<keyword>Kapp</keyword>
<keyword>simon</keyword>
<keyword>recognition</keyword>
<keyword>speech</keyword>
<keyword>voice</keyword>
<keyword>command</keyword>
<keyword>control</keyword>
<keyword>scenarios</keyword>
<keyword>acoustic</keyword>
<keyword>accessibility</keyword>
</keywordset>

</bookinfo>

<chapter id="introduction">
<title>Introduction</title>

<para>
&kmyapplication; is the main front end for the simon open source speech recognition solution.

It is a simond client and provides a graphical user interface for managing the speech model and the commands. Moreover, simon can execute all sorts of commands based on the input it receives from the server: simond.
</para>

<para>
In contrast to existing commercial offerings, simon provides a unique do-it-yourself approach to speech recognition. Instead of predefined, pre-trained speech models, simon does not ship with any model whatsoever. Instead, it provides an easy to use end-user interface to create language and acoustic models from scratch.
</para>

<para>
Additionally the end-user can easily download created use cases from other users and share his / her own.
</para>

<para>
The current release can be used to set up command-and-control solutions especially suitable for disabled people. However, because of the amount of training necessary, continuous, free dictation is neither supported nor reasonable with current versions of simon.
</para>

<para>
Because of it's architecture, the same version of simon can be used with all languages and dialects. One can even mix languages within one model if necessary.
</para>
</chapter>


<chapter id="overview">
<title>Overview</title>

<sect1 id="architecture">
<title>Architecture</title>
<para>
The main recognition architecture of simon consits of three applications.
<itemizedlist>
  <listitem><para>&kmyapplication;</para><para>This is the main graphical interface.</para><para>It acts as a client to the simond server.</para></listitem>
  <listitem><para>simond</para><para>The recognition server.</para></listitem>
  <listitem><para>ksimond</para><para>A graphical front-end for simond.</para></listitem>
</itemizedlist>
</para>

<para>
These three components form a real a client / server solution for the recognition. That means that there is one server (simond) for one or more clients (&kmyapplication;; This application). KSimond is just a front-end for simond which means it adds no functionality to the system but rather provides a way to interact with simond graphically.
</para>

<para>
Additionally to the simon, simond and ksimond other, more specialized applications are also part of this integrated simon distribution.
<itemizedlist>
  <listitem><para>sam</para><para>Provides more in-depth control to your speech model and allows to test the acoustic model.</para></listitem>
  <listitem><para>ssc / sscd</para><para>These two applications can be used to collect large amount of speech samples from different persons more easily.</para></listitem>
</itemizedlist>
</para>

<para>
Please refer to the individual handbooks of those applications for more details.
</para>


<para>
<screenshot>
<screeninfo>Architecture</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="architecture.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>&kmyapplication; is used to create and maintain a representation of your pronunciation and language. This representation is then sent to the server simond which compiles it into a usable speech model.</para>
<para>&kmyapplication; then records sound from the microphone and transmits it to the server which runs the recognition on the received input stream. simond sends the recognition result back to the client (&kmyapplication;).</para>
<para>&kmyapplication; then uses this recognition result to execute commands like opening programs, following links, etc.</para>

<para>simond identifies its connections with a user / password combination which is completely independent from the underlying operating system and its users. By default a standard user is set up in both simon and simond so the typical use case of one simond server per simon client will work "out of the box".</para>
<para> Every simon client logs onto the server with a user / password combination which identifies a unique user and thus a unique speech model. Every user maintains his own speech model but may use it from different computers (different, physical simon instances) simply by accessing the same simond server. One simond instance can of course also serve multiple users.</para>
<para>If you want to open up the server to the internet or use multiple users on one server, you will have to configure simond. Please see the <ulink url="help:/simond">simond manual</ulink> for details.
</para>
</sect1>


<sect1 id="speech_model">
<title>Speech Recognition: Background</title>

<para>
Speech recognition systems take voice input (often from a microphone) and try to translate it into written text. To do that, they rely on statistical representations of human voice. To put it into simple terms: The computer learns how words - or more correctly the sounds that make up those words - sound.
</para>

<para>
A speech model consists of two distinct parts:
<itemizedlist>
  <listitem><para>Language Model</para></listitem>
  <listitem><para>Acoustic Model</para></listitem>
</itemizedlist>
</para>

<sect2 id="language_model">
<title>Language Model</title>

<para>The language model defines the vocabulary and the grammar you want to use.</para>
<para>For more information see the <link linkend="vocabulary">vocabulary section</link> and the <link linkend="grammar">grammar section</link>.</para>
</sect2>

<sect2 id="acoustic_model">
<title>Acoustic Model</title>
<para>The acoustic model represents your pronunciation in a machine readable format.</para>

<para>Let's look at the following sample vocabulary:
<table frame='all'><title>Sample Vocabulary</title>
<tgroup cols='3' align='left' colsep='1' rowsep='1'>
<colspec colname='c1'/>
<colspec colname='c2'/>
<colspec colname='c3'/>
<thead>
<row>
  <entry>Word</entry>
  <entry>Terminal</entry>
  <entry>Pronunciation</entry>
</row>
</thead>
<tbody>
<row>
  <entry>Computer</entry>
  <entry>Noun</entry>
  <entry>k ax m p y uw t er</entry>
</row>
<row>
  <entry>Internet</entry>
  <entry>Noun</entry>
  <entry>ih n t er n eh t</entry>
</row>
<row>
  <entry>Mail</entry>
  <entry>Noun</entry>
  <entry>m ey l</entry>
</row>
<row>
  <entry>close</entry>
  <entry>Verb</entry>
  <entry>k l ow s</entry>
</row>
</tbody>
</tgroup>
</table>
</para>

<para>
The pronunciation of each word is composed of individual sounds which are separated by spaces. For example, the word "close" consists of the following sounds:

<itemizedlist>
  <listitem><para>k</para></listitem>
  <listitem><para>l</para></listitem>
  <listitem><para>ow</para></listitem>
  <listitem><para>s</para></listitem>
</itemizedlist>

The acoustic model uses the fact that spoken words are composed of sounds much like written words are composed of letters. Using this knowledge, we can segment words into sounds (represented by the pronunciation) and assemble them back when recognizing. These building blocks are called "phonemes".
</para>

<para>
Because the acoustic model actually represents how you speak the phonemes of the words, trainings material is shared among all words that use the same phonemes.
</para>
<para>
That means if you add the word "clothes" to the language model, your acoustic model already has an idea how the "clo" part is going to sound as they share the same phonemes ("k", "l", "ow") at the beginning.
</para>
<para>
To train the acoustic model (in other words to tell him how you pronounce the phonemes) you have to "train" words from your language model. That means that simon displays a word which you read out loud. Because the word is listed in your vocabulary, simon already knows what phonemes it contains and can thus "learn" from your pronunciation of the word.
</para>
</sect2>
</sect1>



<sect1 id="scenarios">
<title>Scenarios</title>

<para>
One scenario makes up one complete use case of simon. To control firefox, for example, the user just installs the firefox scenario.
</para>

<para>
Each scenario consists of the following components:
<itemizedlist>
<listitem><para><link linkend="vocabulary">Vocabulary</link></para></listitem>
<listitem><para><link linkend="grammar">Grammar</link></para></listitem>
<listitem><para><link linkend="training">Trainingstexts</link></para></listitem>
<listitem><para><link linkend="commands">Commands</link></para></listitem>
</itemizedlist>
</para>

<para>
Scenarios only cover the language model of the recognition system, the acoustic model is completely independent.
</para>

<para>
However, in most cases scenarios are tailored to work best with a specific <link linkend="base_model">base model</link> to avoid <link linkend="phoneme_set_issues">issues with the phoneme set</link>.
</para>

<para>
Because scenarios are not specifically bound to the acoustic model, they can be shared and exchanged between different simon users without problems. To accomodate this community based repository pool, a <ulink url="http://kde-files.org/index.php?xcontentmode=692">category for simon scenarios has been created on kde-files.org</ulink> where the scenarios, which are just simple text files (XML format), can be exchanged easily.
</para>

<para>
For information on how to use scenarios in simon, please refer to the <link linkend="scenarios_use">Scenario section in the Use simon chapter</link>.
</para>

</sect1>

<sect1 id="base_model">
<title>Base models</title>
<para>
Base models are already generated, most often speaker independent, acoustic models that can be used with simon.
</para>
<para>
Using base models, the user can greatly reduce or even eliminate the need for personalized training. When using a static base model (see below), installation of the HTK is not necessary.
</para>

<para>
Base models usable by simon consists of four files which you will find in archive when you download base models from their respective website.

<itemizedlist>
<listitem><para>hmmdefs</para></listitem>
<listitem><para>tiedlist</para></listitem>
<listitem><para>macros</para></listitem>
<listitem><para>stats</para></listitem>
</itemizedlist>
</para>

<para>
The latter two files, macros and stats, are not required when using a static base model and might, in that case, be replaced with empty files if they are not available.
</para>

<sect2 id="base_model_get">
<title>Where to get base models</title>

<para>
To keep this list up to date, please refer to <ulink url="http://www.simon-listens.org/wiki/index.php/English:_Base_models#Where_to_get_base_models">the list in our online wiki</ulink>.
</para>
</sect2>

<sect2 id="base_model_types">
<title>Types of base models</title>

<para>
There are three types of base models:

<itemizedlist>
<listitem><para>Static base model</para>
</listitem>
<listitem><para>Adapted base model</para>
</listitem>
<listitem><para>User generated model</para>
</listitem>
</itemizedlist>
</para>

<para>
For information on how to use base models in simon, please refer to the <link linkend="base_model_use">Base Models section in the Use simon chapter</link>.
</para>

<sect3 id="base_model_static">
<title>Static base model</title>

<para>Static base models simply use a pre-compiled acoustic model without modifying it.</para>

<para>Any training data collected through simon will not be used to improve the recognition accuracy.</para>

<para>This type of model <emphasis>does not</emphasis> require the HTK to be installed.</para>

</sect3>

<sect3 id="base_model_adapted">
<title>Adapted base model</title>

<para>By adapting a pre-compiled acoustic model you can improve accuracy by adapting it to your voice.</para>

<para>Collected training data will be compiled in a adaption matrix which will then be applied to the selected base model.</para>

<para>This type of model <emphasis>does</emphasis> require the HTK to be installed.</para>

</sect3>

<sect3 id="base_model_user_generated">
<title>User generated model</title>

<para>When using user generated models, the user is responsible for training his own model. No base model will be used.</para>

<para>The training data will be used to compile your own acoustic model allowing you to create a system which directly reflects your voice.</para>

<para>This type of model <emphasis>does</emphasis> require the HTK to be installed.</para>
</sect3>

</sect2>


<sect2 id="phoneme_set_issues">
<title>Phoneme set issues</title>
<para>Because the statistical comparison happens <link linkend="acoustic_model">at phoneme level</link> the base models describe how these phonemes sound.</para>

<para>Your scenarios (language model) on the other hand describe what phonemes a word is composed of.</para>

<para>In order for this association to work, both your scenarios and your base model need to use the same set of phonemes.</para>

<para>If you design a new scenario it is therefore a good idea to use the dictionary that was used to create the base model as shadow dictionary. This way simon will suggest the "correct" phonemes when adding the words automatically.</para>

<para>
If you try to use scenarios designed for a different phoneme set (different base model) then you will get an error when starting the recognition listing the affected phonemes and words. To fix this, either transcribe the words according to the base models phoneme set, use a different base model or use an user generated model.
</para>
</sect2>
</sect1>
</chapter>



<chapter id="guidelines">
<title>Guidelines</title>
<para>This chapter lists some general guidelines that are relevant for different parts of simon.</para>

<sect1 id="recording">
<title>Recordings</title>
<para>If you are using user generated or adapted models, simon builds it's acoustic model based on transcribed samples of the users voice. Because of this, the recorded samples are of vital importance for the recognition performance.</para>

<sect2 id="volume">
<title>Volume</title>
<para>It is important that you check your microphone volume before recording any samples.</para>

<sect3 id="volume_calibration">
<title>simon Calibration</title>

<para>The current version of &kmyapplication; includes a simple way of ensuring that your volume is configured correctly.</para>

<para>
<screenshot>
<screeninfo>simon Volume Calibration</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="volume_calibration.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>By default the volume calibration is displayed before starting any recording in &kmyapplication;.</para>

<para>To calibrate simply read the text displayed.</para>

<para>The calibration will monitor the current volume and tell you to either raise or lower the volume but you have to do that manually in your systems audio mixer. Once you changed the volume in any way (while the calibration is running), press the "Volume changed" button next to the affected device. This will reset the volume status.
</para>

<para>During calibration, try to talk normally. Don't yell but don't be overly quiet either. Take into account that you should generally use the same volume setting for all your training and for the recognition too. You might speak a little bit louder (unconsciously) when you are upset or at another time of the day so try to raise your voice a little bit to anticipate this. It is much better to have a little quieter samples than to start clipping.</para>

<para>In the &kmyapplication; settings, both the text displayed and the levels considered correct can be changed. If you leave the text empty, the default text will be displayed. In the options you can also deactivate the calibration completely. See the <link linkend="soundconfiguration_training">training section</link> for more details.</para>
</sect3>

<sect3>
<title>Audacity Calibration</title>

<para>Alternatively you can use an audio editing tool like the free <ulink url="http://audacity.sourceforge.net">Audacity</ulink> to monitor the recording volume.</para>

<para>
Too quiet:
<screenshot>
<screeninfo>Volume: Too quiet</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="too_quiet.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
Too loud:
<screenshot>
<screeninfo>Volume: Too loud</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="too_loud.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
Perfect volume:
<screenshot>
<screeninfo>Perfect volume</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="volume_perfect.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect3>

</sect2>

<sect2 id="silence">
<title>Silence</title>
<para>To help simon with the automatic segmentation it is recommended to leave about one or two seconds of silence on the recording before and after reading the prompted text.</para>

<para>
Current simon versions include a graphical notice on when to speak during recording. The message will tell the user to wait for about half a second:
<screenshot>
<screeninfo>Please wait</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="pause_control_wait.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
... before telling the user to speak:
<screenshot>
<screeninfo>Please speak</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="pause_control_speak.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
This method of visual feedback proved especially valuable when recording with people who can't read the prompted text for themselves and therefore need someone to tell them what they have to say. The colorful visual cue tells them when to start repeating what the facilitator said without the need of unreliable hand gestures.
</para>
</sect2>


<sect2 id="microphone">
<title>Microphone</title>
<para>For simon to work well, a high quality microphone is recommended.</para>

<para>However, even relatively cheap headsets (around 30 Euros) achieve very good results - magnitudes better than internal microphones.</para>

<para>For maximum compatability we recommend USB headsets as they usually support the necessary samplerate of 16 kHz, are very well supported from both Microsoft Windows as well as GNU/Linux and normally don't require special, proprietary drivers to operate.</para>
</sect2>


<sect2 id="sample_quality_assurance">
<title>Sample Quality Assurance</title>
<para>simon will check each recording against certain criteria to ensure that the recorded samples are not errenous or of poor quality.</para>

<para>If simon detects a problematic sample, it will warn the user to re-record the sample.</para>

<para>
Currently, simon checks the following criteria:
<itemizedlist>
<listitem>
<para>Sample peak volume</para>
<para>If the volume is too loud and the microphone started to "clip" (<ulink url="http://en.wikipedia.org/wiki/Clipping_%28audio%29">Clipping on wikipedia</ulink>), simon will display a warning message urging the user to lower the microphone volume.</para>
</listitem>
<listitem>
<para>Signal to noise ratio (SNR)</para>
<para>simon will automatically determine the signal to noise ratio of each recording. If the ratio is below a configurable threshold, a warning message will be displayed.</para>
<para>The default value of 5000 % means that for simon to accept a sample as correctly recorded the peak volume has to be 500 times louder than the noise baseline (lowest average over 50 ms).</para>
<para>
Often this can be a result of either a very low quality microphone, high levels of ambient noise or a low microphone gain coupled with a "microphone boost" option in the system mixer.
</para>
</listitem>
</itemizedlist>
</para>


<para>
SNR warning message triggered by an empty sample; This information dialog is displayed when clicking on the "More information" button visible in the background.
<screenshot>
<screeninfo>Empty sample triggering the SNR warning</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="snr_warning.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect2>
</sect1>

</chapter>


<chapter id="using-simon">
<title>Using &kmyapplication;</title>

<para>The following sections will describe how to use simon.</para>

<sect1 id="main_window">
<title>The simon Main Window</title>
<para>
<screenshot>
<screeninfo>A screenshot of &kmyapplication;</screeninfo>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="screenshot_main.png" format="PNG"/>
	  </imageobject>
	  <textobject>
	    <phrase>Screenshot</phrase>
	  </textobject>
	</mediaobject>
</screenshot>

The simon main window provides quick access to most of its features through the main toolbar.
</para>

<para>There are 9 main actions listed:
<itemizedlist>
<listitem>
<para>simond connection</para>
<para>This menu item has several states:
<itemizedlist>
<listitem><para>Connect</para>
<para>When simon is not connected to simond the option says "Connect". When activated, simon will start to connect to simond and change to the "Connecting" state.</para>
<para>Upon connecting to the server from simon for the first time, you might be prompted for a username and a password. If you haven't done so already, set up a user for simond (see the <ulink url="help:/simond">simond manual</ulink> for details) before continuing and enter the same username and corresponding password in the login dialog from simon. If you choose to store the password, you can still change it in the <link linkend="configure_server_general">server configuration</link> at any time.</para>
</listitem>
<listitem><para>Connecting</para>
<para>When simon is currently connecting to the configured simond server(s) the option says "Connecting" and is pressed down. When activated, simon will stop trying to connect to simond and go back to the "Connect" state.</para>
</listitem>
<listitem><para>Activate</para>
<para>When simon established a connection to the server the option will say "Activate" and will not be pressed down. When activated (or automatically when simon is configured to automatically start the recognition when it is available) simon will try to start the recognition.</para>
<para>An option to close the connection to simond ("Disconnect") is available through the small down-arrow next to it.</para>
</listitem>
<listitem><para>Activated</para>
<para>When simon established a connection to the server and has successfully started the recognition the option will say "Activated" and will be pressed down. When activated simon will deactivate the recognition but not close the connection to simond - it changes back to the previous "Activate" state.</para>
<para>An option to close the connection to simond ("Disconnect") is available through the small down-arrow next to it.</para>
</listitem>
</itemizedlist>
</para>
</listitem>


<listitem>
<para>Add Word</para>
<para>Displays the <link linkend="add_word">add word wizard</link>.</para>
</listitem>

<listitem>
<para>Vocabulary</para>
<para>Displays the <link linkend="vocabulary">vocabulary</link>.</para>
</listitem>

<listitem>
<para>Grammar</para>
<para>Displays the <link linkend="grammar">grammar</link>.</para>
</listitem>

<listitem>
<para>Training</para>
<para>Displays the <link linkend="training">training</link>.</para>
</listitem>

<listitem>
<para>Commands</para>
<para>Displays the <link linkend="commands">commands</link>.</para>
</listitem>

<listitem>
<para>Synchronize</para>
<para>When connected to the simond, this option will be available.</para>
<para>simon creates the speech input files which are then compiled and used by the simond server (see the <link linkend="architecture">section architecture for more details</link>).</para>
<para>The process of sending the speech input files, compiling them and receiving the compiled versions is called "synchronization". By default, simon will initiate a synchronization immediately after the connection has been established and whenever the model changes (please see the <link linkend="configure_synchronization">configure synchronization section</link> for information on how to change that).</para>
<para>Using this menu option the synchronization can be triggered manually at any time.</para>
</listitem>

<listitem>
<para>Scenario selection</para>
<para>This selection box allows you to select the currently displayed scenario. Each subsection (vocabulary, grammar, commands, training) will then adapt to the currently displayed scenario. Selecting a different scenario here does not affect the recognition.</para>
</listitem>

<listitem>
<para>Manage scenarios</para>
<para>Shows the <link linkend="scenarios_use">scenario management dialog</link>. There you can manage your scenarios and change the options of the scenario selection box.</para>
</listitem>


</itemizedlist>
</para>

<para>
The simon main window can be hidden at any time by clicking on the simon logo in the system tray (usually next to the system clock in the task bar) which will minimize simon to the tray. Click it again to show the main window again.</para>

</sect1>


<sect1 id="needed_parts">
<title>Required Resources for a Working &kmyapplication; Setup</title>

<note>
<para>
For more information about speech models, please refer to the <link linkend="speech_model">Speech Recognition: Background section in the Overview chapter</link>.
</para>
</note>

<para>To get simon to recognize speech and react to it you need a speech model. </para>

<para>Speech models describe how your voice sounds, what words exist, how they sound and what word combination ("sentences" or "structures") exist.</para>

<para>A speech model basically consists of two parts: 
<itemizedlist>
<listitem><para>Language model: Describes all existing words and what sentences are grammatically correct </para></listitem>
<listitem><para>Acoustic model: Describes how words sound </para></listitem>
</itemizedlist>
</para>

<para>You need both these components to get simon to recognize your voice.</para>

<sect2>
<title>Language Model</title>
<para>
In most cases you only need to <link linkend="scenarios_use">install the appropriate scenario</link> for your use case to set up your language model. 
</para>
<para>
To create your own language model, you can use simon to <link linkend="add_word">add</link> / <link linkend="edit_word">edit</link> / <link linkend="remove_word">remove</link> words and <link linkend="grammar">grammar structures</link>.
</para>
<para>
To make the adding of words easier, you can import a <link linkend="shadow_dictionary">shadow dictionary</link>.
</para>

</sect2>

<sect2>
<title>Acoustic Model</title>
<para>
To create your own acoustic model you can simple <link linkend="training">read the trainings texts</link> that come with your selected scenarios a couple of times. 
</para>
<para>
If you are creating your own scenario you can easily <link linkend="training">create trainingstexts yourself</link>.
</para>
<para>
You can, however use <link linkend="base_model_use">static or adapted base models</link> to avoid using the HTK or to improve the recognition rate.
</para>
</sect2>
</sect1>

<sect1 id="first_run">
<title>First run wizard</title>

<para>On the first start of simon, this wizard is displayed to guide you through the initial configuration of &kmyapplication;.</para>

<para>
  <screenshot>
    <screeninfo>First run: Welcome</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="first_run_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>The configuration consists of five easy steps which are outlined below. You can skip each step and even the whole wizard if you want to - in that case, the system will be set up with default values.</para>

<sect2 id="first_run_scenarios">
<title>Scenarios</title>

<para>In this step you can download <link linkend="scenarios">scenarios</link> from the internet and import them into simon.</para>

<para>
  <screenshot>
    <screeninfo>First run: Scenarios</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="first_run_2.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>Pressing <guibutton>Get scenarios</guibutton> displays the download dialog.</para>

<para>
  <screenshot>
    <screeninfo>First run: Get scenarios</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="get_hot_new_scenarios_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>If you import some scenarios here (or later on in the <link linkend="scenarios_use">scenario management dialog</link>) you don't need to set up the vocuablary, grammar, commands, etc. for yourself. Especially for new users it is recommended to try some scenarios first to see how the system works before diving into configuring it exactly for your use case.</para>

</sect2>


<sect2 id="first_run_basemodels">
<title>Base models</title>

<para>In this step you can set up simon to use <link linkend="base_model">base models</link>.</para>

<para>
  <screenshot>
    <screeninfo>First run: Base models</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="first_run_3.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>The configuration page opened is the same one that is described in the <link linkend="base_model_use">base model usage</link> section.</para>

<para>
  <screenshot>
    <screeninfo>First run: Configure base models</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="first_run_base_model.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>After completing or aborting the first run wizard you can change configuration options defined here in the <link linkend="base_model_use">simon configuration</link>.</para>

</sect2>

<sect2 id="first_run_server">
<title>Server</title>

<para>Internally, simon is a <link linkend="architecture">server / client application</link>. If you want to take advantage of a network based installation, you can provide the server address here.</para>

<para>
  <screenshot>
    <screeninfo>First run: Server</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="first_run_4.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>The default configuration is sufficient for a "normal" installation and will assume that you use a local simond server that will be automatically be started and stopped with simon.</para>

<para>After completing or aborting the first run wizard you can change configuration options defined here in the <link linkend="configure_server">server configuration</link>.</para>

</sect2>

<sect2 id="first_run_sound_configuration">
<title>Sound configuration</title>

<para>Because simon recognizes sound from one or more microphones, you have to tell simon which devices you want to use for recognition and training.</para>

<para>
  <screenshot>
    <screeninfo>First run: Sound configuration</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="first_run_5.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>simon can use one or more input- and output devices for different tasks. You can find more information about simons multiple device capabilities in the <link linkend="soundconfiguration_device">simon sound configuration section</link>.</para>

<para>If you don't set at least one input device to be used for recognition, you will not be able to activate simon.</para>

<para>When the option <guibutton>Default to power training</guibutton> is selected, simon will, when training, automatically start- and stop the recording when displaying and hiding (respectively) the recording prompt. This option only sets the default value of the option, the user can change it at any time before beginning a training session.</para>

<para>After completing or aborting the first run wizard you can change configuration options defined here in the <link linkend="soundconfiguration_device">sound configuration</link>.</para>

</sect2>

<sect2 id="first_run_volume_calibration">
<title>Volume calibration</title>

<para>For simon to work correctly, you need to configure your microphones volume to a sensible level.</para>

<para>
  <screenshot>
    <screeninfo>First run: Volume calibration</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="first_run_6.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>For more details on this, please see the <link linkend="volume_calibration">Volume Calibration section</link> in the <link linkend="guidelines">Guidelines chapter</link>.</para>

</sect2>

</sect1>

<sect1 id="scenarios_use">
<title>Scenarios</title>
<para>This section describes how to use scenarios from within simon. For <link linkend="scenarios">general information about scenarios, please refer to the chapter background</link>.</para>

<sect2 id="using_scenarios">
<title>Using scenarios</title>
<para>Beginning with &kmyapplication; 0.3, each word you add will be added to the currently active scenario. The same goes for grammar sentences, commands, etc.</para>
<para>Using scenarios then becomes just using &kmyapplication; as you did in 0.2.</para>

<para>Per default, simon ships with an empty scenario names "Standard", so your configuration will be stored in this scenario.</para>

<para>To select which of your currently active scenarios should be changed (for example before adding new words), just select it from the drop down list in the upper right corner of the simon main window.</para>

<para>
  <screenshot>
    <screeninfo>Scenario toolbar</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="scenarios_use_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

  <para>To change the available options, click on the <guibutton>Manage scenarios</guibutton> button right next to it or use the menu entry <guimenu>Scenarios</guimenu> > <guimenuitem>Manage scenarios</guimenuitem>.</para>
</sect2>

<sect2 id="administrating_scenarios">
<title>Managing scenarios</title>
<para>The scenario management dialog allows you to load scenarios from your scenario pool as well as to import and export scenarios to files or directly from / to an online repository.</para>

<para>
  <screenshot>
    <screeninfo>Manage scenarios</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="manage_scenarios_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>To load or unload a scenario you can use the arrow buttons between the two lists or simply double click the option you want to load / unload.</para>
<para>More information about individual scenarios can be found in the tooltips of the list items.</para>

<sect3 id="add_scenario">
  <title>Adding a new Scenario</title>
  <para>To add a new scenario, select the <guibutton>Add</guibutton> button. A new dialog will be displayed.</para>

<para>
  <screenshot>
    <screeninfo>Add scenario</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="add_scenario.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>When creating a new scenario, please give it a descriptive name. For the later upload on <ulink url="http://kde-files.org/index.php?xcontentmode=692">KDE files</ulink> we would kindly ask you to follow a certain naming scheme altough this is of course not a requirement: "[&lt;language&gt;/&lt;base model&gt;] &lt;name&gt;". If, for example you create a scenario in English that works with the Voxforge base model and controls Mozilla Firefox this becomes: "[EN/VF] Firefox". If your scenario is not specifically tailored to one phoneme set (base model), just omit the second tag like this: "[EN] Firefox".</para>

<para>The scenario version is just an incremental version number that makes it easier to distinguish between different revisions of a scenario.</para>

<para>If your scenario needs a specific feature of simon (for example because you use a new plugin), you can define minimum and maximum version numbers of simon here.</para>

<para>The license of your scenario can be set through the drop down. You can of course also add an arbitrary license text directly in the input field.</para>

<para>You can then add your name (or alias) to the list of scenario authors. There you will also be asked for contact information. This field is purely provided as a convenient way to contact a scenario author for changes, problems, fanmail etc. If you don't feel comfortable providing your e-Mail address you can simply enter a dash "-" denoting that you are not willing to divulge this information.</para>

</sect3>

<sect3 id="edit_scenario">
<title>Edit Scenario</title>
<para>To edit scenarios, just select "Edit" from the "Manage scenarios" dialog.</para>
<para>The dialog works exactly the same as the <link linkend="add_scenario">add scenario</link> dialog.</para>
</sect3>

<sect3 id="delete_scenario">
<title>Delete Scenario</title>

<para>To delete a scenario, select the scenario and click the "Delete" button.</para>

<para>Because scenarios are synchronized with the recognition server, you can restore deleted scenarios through the <link linkend="configure_synchronization">model synchronization backup</link>.</para>
</sect3>

<sect3 id="import_scenario">
  <title>Import Scenario</title>
  <para>Scenarios can be imported from a local file in simons XML scenario file format but can also be directly downloaded and imported from the internet.</para>

  <para>When downloading scenarios, the list of scenarios is retrieved from <ulink url="http://kde-files.org/index.php?xcontentmode=692">simon Scenarios</ulink> subsection of the OpenDesktop site <ulink url="http://kde-files.org">KDE-files.org</ulink>.</para>

<para>
  <screenshot>
    <screeninfo>Download scenarios</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="get_hot_new_scenarios_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

  <para>If you create a scenario that might be valuable for other simon users, please consider <link linkend="export_scenario">uploading it</link> to this online repository and help other simon users.</para>
</sect3>

<sect3 id="export_scenario">
  <title>Export Scenario</title>
  <para>Scenarios can be exported to a local file in simons XML scenario file format and directly uploaded to the <ulink url="http://kde-files.org/index.php?xcontentmode=692">simon Scenarios</ulink> subsection of the OpenDesktop site <ulink url="http://kde-files.org">KDE-files.org</ulink>.</para>

  <para>To upload to OpenDesktop sites, you need an account on the site. <ulink url="http://opendesktop.org/usermanager/new.php">Registration</ulink> is very easy and of course free of charge.</para>

  <para>simon allows you to upload new content directly from within simon (<guibutton>Export</guibutton> > <guimenuitem>Publish</guimenuitem>).</para>

<para>
  <screenshot>
    <screeninfo>Upload scenario wizard: 1 of 4</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="upload_scenario_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>
  <screenshot>
    <screeninfo>Upload scenario wizard: 2 of 4</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="upload_scenario_2.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>
  <screenshot>
    <screeninfo>Upload scenario wizard: 3 of 4</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="upload_scenario_3.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>
  <screenshot>
    <screeninfo>Upload scenario wizard: 4 of 4</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="upload_scenario_4.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

  <para>To use this functionality, simply enter your account credentials in the <link linkend="social_desktop_config">social desktop configuration</link> in the simon configuration.</para>
</sect3>

</sect2>
</sect1>

<sect1 id="base_model_use">
<title>Base models</title>
<para>This section describes how to use base models from within simon. For <link linkend="base_model">general information about base models, please refer to the chapter background</link>.</para>

<para>To configure simon to use base models, simply select the base model type you want to use and point simon to the valid files in simons configuration: <guimenu>Settings</guimenu> > <guimenuitem>Configure simon</guimenuitem> > <guimenuitem>Model Settings</guimenuitem> </para>

<para>
  <screenshot>
    <screeninfo>Base model settings</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="base_model_settings_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>Load the files with the appropriate "Load" button next to the file you want to set. The files will be copied to an internal location so the source files can be removed once you have selected them here.</para>

<para>For static models you don't need macros or stats but simon will not start a model compilation (which is needed even for setups using static base models to generate the language model) without them. If your base model doesn't provide them you can simply point simon to empty files instead.</para>

</sect1>


<sect1 id="vocabulary">
<title>Vocabulary</title>
<para>The vocabulary lets the user manage the available words.</para>

<sect2 id="vocabulary_theory">
<title>General</title>
<para>
The vocabulary defines what words the recognition process should recognize. Every word you want to be able to use with simon should be contained in your vocabulary.
</para>
<para>
One entry in the vocabulary defines exactly one "word". In contrast to the common use of the word "word", in simon "word" means one unique combination of the following:
<itemizedlist>
  <listitem><para>Wordname</para><para>(The written word itself)</para></listitem>
  <listitem><para>Terminal</para><para>(Grammatical category; For example: "Noun", "Verb", etc.)</para></listitem>
  <listitem><para>Pronunciation</para><para>(How the word is pronounced; simon accepts any kind of phonetic as long as it does not use special characters or numbers)</para></listitem>
</itemizedlist>

That means that plurals or even different cases are different "words" to simon.
This is an important design decision to allow more control when using a sophisticated grammar.
</para>

<para>
In general, it is advisable to keep your vocabulary as sleek as possible. The more words, the higher the chance that simon might misunderstand you.
</para>

<para>
Example vocabulary (please note that the terminals here are deliberately set to Noun / Verb to help the understanding; Please to refer to the <link linkend="grammar">grammar section</link> why this might not be the best idea):
<table frame='all'><title>Sample Vocabulary</title>
<tgroup cols='3' align='left' colsep='1' rowsep='1'>
<colspec colname='c1'/>
<colspec colname='c2'/>
<colspec colname='c3'/>
<thead>
<row>
  <entry>Word</entry>
  <entry>Terminal</entry>
  <entry>Pronunciation</entry>
</row>
</thead>
<tbody>
<row>
  <entry>Computer</entry>
  <entry>Noun</entry>
  <entry>k ax m p y uw t er</entry>
</row>
<row>
  <entry>Internet</entry>
  <entry>Noun</entry>
  <entry>ih n t er n eh t</entry>
</row>
<row>
  <entry>Mail</entry>
  <entry>Noun</entry>
  <entry>m ey l</entry>
</row>
<row>
  <entry>close</entry>
  <entry>Verb</entry>
  <entry>k l ow s</entry>
</row>
</tbody>
</tgroup>
</table>
</para>

<sect3 id="active_dictionary">
<title>Active Dictionary</title>
<para>The vocabulary used for the recognition is referred to as active dictionary or active vocabulary.</para>
</sect3>

<sect3 id="shadow_dictionary">
<title>Shadow Dictionary</title>
<para>
As said above, the user should keep his vocabulary / dictionary as lean as possible. However, as a word in your vocabulary has to also have information about it's pronunciation, it would also be good to have large dictionary where you could look up the pronunciation and other characteristics of the words.
</para>
<para>
simon provides this functionality. We refer to this large reference dictionary as "shadow dictionary". This shadow dictionary is not created by the user but can be imported from various sources.
</para>
<para>
As simon is a multi-language solution we do not ship shadow dictionaries with simon. However, it is very easy to import them yourself using the import dictionary wizard. This is described in the <link linkend="import_dictionary">Import Dictionary section</link>.
</para>
</sect3>
</sect2>



<sect2>
<title>Maintaining the Vocabulary</title>
<para>
simon provides a "Vocabulary" menu which lists the current vocabulary.

<screenshot>
<screeninfo>simons Vocabulary</screeninfo>
<mediaobject>
<imageobject>
  <imagedata fileref="vocabulary.png"   format="PNG"/>
</imageobject>
</mediaobject>
</screenshot>
</para>

<para>Per default, the active vocabulary is shown. To display the shadow vocabulary select the tab <guilabel>Shadow Vocabulary</guilabel>.</para>

<para>
Every word states it "recognition rate" which at the moment is just a counter of how often the word has been recorded (alone or together with other words).
</para>
<para>
When this number is only one or zero the word entry is colored red (1: light red; 0: dark red). This is a visual warning. When a word contains a phoneme combination that is not covered by any other word and the word with this unusual phoneme combination is never recorded (recognition rate = 0), the model <emphasis>will</emphasis> fail to compile. However, simon will display an appropriate error message when the compilation of the model fails because of such an issue. In general it is a good idea to record each word at least once or twice (at best when <link linkend="add_word_record">adding the word</link>) to avoid such problems.
</para>
<para>

<screenshot>
<screeninfo>Shadow Vocabulary</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="vocabulary1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>


</para>

</sect2>


<sect2 id="add_word">
<title>Adding Words</title>
<para>To add new words to the active vocabulary, use the add word wizard.</para>
<para>Adding words to simon is basically a two step procedure:
<itemizedlist>
<listitem><para>Defining the word</para></listitem>
<listitem><para>Initial training</para></listitem>
</itemizedlist>
</para>

<sect3 id="add_word_define">
<title>Defining the Word</title>
<para>
Firstly, the user is asked which word he wants to add.

<screenshot>
<screeninfo>Select the word to add</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="add_word_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
When the user proceeds to the next page, simon automatically tries to find as much information about the word in the <link linkend="shadow_dictionary">shadow dictionary</link> as possible.
</para>

<para>
If the word is listed in the shadow dictionary, simon automatically fills out all the needed fields (Terminal and Pronunciation).

<screenshot>
<screeninfo>Fields automatically filled out by the Shadow Dictionary</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="add_word_2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
<para> All suggestions from the shadow dictionary are listed in the table "Similar words". Per default only exact word matches are shown. However, this can be changed by checking the "Also show non-perfect matches" checkbox below the suggestion table. Using similar words you can quickly deduce the correct pronunciation of the word you are actually trying to add. See <link linkend="manual_transcription">below</link> for details.</para>

<para>
Of course this really depends on your shadow dictionary. If the shadow dictionary does not contain the word you are trying to add, the required fields have to be filled out manually.</para>

<para>
Some dictionaries that can be imported with simon (SPHINX, HTK) do not differentiate between upper and lower case. Suggestions based on those dictionaries will always be uppercase. You are of course free to change these suggestions to the correct case.
</para>

<para>Some dictionaries that can be imported with simon (SPHINX, PLS and HTK) provide no grammatical information at all. These will assign all the words to the terminal "Unknown". You should change this to something appropriate when adding those words.
</para>

<sect4>
<title>Manually Selecting a Terminal</title>
<para>
The terminal of the word is defined as the grammatical category the word belongs to. This might be "Noun", "Verb" or completely new categories like "Command". For more information see the <link linkend="grammar">grammar section</link>.</para>
<para>
The list contains all terminals used in both your active and your shadow lexicon and in your grammar.
</para>
<para>
You can add new terminals to the drop-down menu by using the green plus sign next to it.
</para>
</sect4>

<sect4 id="manual_transcription">
<title>Manually Providing the Phonetic Transcription</title>
<para>
The pronunciation is a bit trickier. simon does not need a certain type of phonetics so you are free to use any method as long as it uses only ASCII characters and no numbers. However, if you want to use a shadow dictionary and want to use it to it's full potential you should use the same phonetics as the shadow dictionary.
</para>

<para>
If you don't know how to transcribe a word yourself you can easily use your shadow dictionary to help you with the transcription - even if the word is not listed in it. Let's say we want to add the word "Firefox" (to launch firefox) which is of course not listed in our shadow dictionary.
</para>

<para>
(I imported the English voxforge HTK lexicon available from <ulink url="http://voxforge.org/home/downloads">voxforge</ulink> as a shadow dictionary.)
</para>

<para>
"Firefox" is not listed in our shadow dictionary so we don't get any suggestion at all.
<screenshot>
<screeninfo>Adding an unknown word</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="add_word_2_2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
However, we know that firefox sounds like "fire" and "fox" put together. So let's just open the vocabulary (you can keep the wizard open) by selecting "Vocabulary" from your simon main toolbar.
</para>
<para>
  Switch to the shadow vocabulary by clicking on the tab <guilabel>Shadow Vocabulary</guilabel>.</para>
<para>
Use the "Filter"-Box above the list to search for "Fire":

<screenshot>
<screeninfo>Adding an unknown word: Search for the Pronunciation</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="add_word_2_3.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
We can see, that the word "Fire" is transcribed as "f ay r". Now filter for "fox" instead of "Fire" and we can see that "Fox" is transcribed as "f ao k s". We can assume, that firefox should be transcribed as "f ay r f ao k s".
</para>

<para>
Using this approach of deducing the pronunciation from parts of the word has the distinct advantage that we not only get a high quality transcription but also automatically use the same phoneme set as the other words which were correctly pulled out of the shadow dictionary.
</para>

<para>
We can now enter the pronunciation and change the terminal to something appropriate.
<screenshot>
<screeninfo>Completely defined word</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="add_word_2_4.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

</sect4>

</sect3>

<sect3 id="add_word_record">
<title>Training the Word</title>
<para>
  To complete the wizard we can now train the word twice. If you don't want to do this or for example use a <link linkend="base_model_static">static base model</link>, you can skip these two pages.
</para> 

<para>
  Because you are about to record some training samples, simon will display the volume calibration to make sure that your microphone is set up correctly. For more information please refer to the <link linkend="volume_calibration">volume calibration section</link>
</para>

<para>
simon will try to prompt you for real-world examples. To do that, simon will automatically fetch grammar structures using the terminal of the word and substitute the generic terminals with example words from your active lexicon.
</para>
<para>
For example: You have the grammar structure "Trigger Command" and have the word "Computer" of the terminal "Trigger" in your vocabulary. You then add a new word "Firefox" of the terminal "Command". simon will now automatically prompt you for "Computer Firefox" as it is - according to your grammar - a valid sentence.
</para>
<para>
If simon is unable to find appropriate sentences using the word (i.e.: No grammar, not enough words in your active lexicon, etc.) it will just prompt you for the word alone.
</para>
<para>
Although simon ensures that the automatically generated examples are valid, you can always override it's suggestion. Just switch to the "Examples" tab on the "Define Word" page.
<screenshot>
<screeninfo>Editing word examples</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="add_word_3.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
You are free to change those examples to anything you like. You can even go so far and use words that are not yet in your active lexicon as long as you add them before you synchronize the model, although this is not recommended.
</para>

<para>
All that is left is to record the examples.
<screenshot>
<screeninfo>Recording</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="add_word_4.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
<para>Make sure you follow the guidelines listed in the <link linkend="recording">recording section</link>.</para>
</sect3>

</sect2>

<sect2 id="edit_word">
<title>Editing a word</title>
<para>
  To edit a word, simply select it from the vocabulary, and click on <guibutton>Edit word</guibutton>.
</para>

<para>
  <screenshot>
    <screeninfo>Edit word</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="edit_word.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>
  There you can change name, terminal and pronunciation of the selected word.
</para>
</sect2>

<sect2 id="remove_word">
<title>Removing a word</title>

<para>To remove a word from your language model, select it in the vocabulary view and click on "Remove selected word".

<screenshot>
<screeninfo>Recording</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="delete_word.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot></para>

<para>
The dialog offers four choices:
<itemizedlist>
  <listitem>
    <para>Move the word to the "Unused" terminal.</para>
    <para>Because you (hopefully) don't use the terminal "Unused" in your grammar, the word will no longer be considered for recognition. In fact, it will be removed from the active vocabulary before compiling the model because no grammar sentence references it.</para>
    <para>If you want to use the terminal "Unused" in your grammar, you can of course use a different terminal for unused words. Just set the terminal through the <link linkend="edit_word">Edit word</link> dialog.</para>
    <para>To use the word again, just set the right terminal again. No data will be lost.</para>
  </listitem>
  <listitem>
    <para>Move the word to the shadow lexicon</para>
    <para>This will remove the selected word from the active lexicon (and thus from the recognition) but will keep a copy in the shadow vocabulary. All the recordings containing the word will be preserved.</para>
    <para>To use the word again, add it again to the active vocabulary. When adding a "new" word with the same name the values of the moved word will be suggested to you. Therefore, no data will be lost.</para>
  </listitem>
  <listitem>
    <para>Delete the word but keep the samples</para>
    <para>Removes the word completely but keeps the associated samples. Whenever you add another word with the same word name the samples will be re-associated.</para>
    <para>Be careful with this option as the new word you add again might be transcribed differently and this difference can not be taken into account automatically (simon will then try to force the new transcription on the old recordings during the model compilation).</para>
    <para>Do not use this option if the samples you recorded for this word were errenous.</para>
  </listitem>
  <listitem><para>Remove the word completely</para>
  <para>Just remove the word. All the recordings containing the word will be removed too.</para>
  <para>This option leaves no trace of neither the word itself nor the associated samples.</para>
  <para>Because samples are global (not assigned to scenarios), even samples recorded from trainings session of other scenarios might be removed as well if they contain the word. Use this option carefully.</para>
  </listitem>
</itemizedlist>
</para>

</sect2>

<sect2 id="special_training_stub">
<title>Special Training</title>
<para>Please see the <link linkend="special_training">special training section in the training section</link>.</para>
</sect2>


<sect2 id="import_dictionary">
<title>Importing a Dictionary</title>
<para>
simon provides the functionality to import large dictionaries as a reference. This reference dictionary is called <link linkend="shadow_dictionary">shadow dictionary</link>.
</para>
<para>
When the user <link linkend="add_word">adds a new word</link> to the model, he has to define the following characteristics to define this word:
<itemizedlist>
  <listitem><para>Wordname</para></listitem>
  <listitem><para>Terminal</para></listitem>
  <listitem><para>Phonetic definition</para></listitem>
</itemizedlist>
</para>
<para>
These characteristics are taken out of the shadow dictionary if it contains the word in question. A large, high quality shadow dictionary can thus help the user to easily add new words to the model without keeping track of the phoneme set or - in many cases - even let him forget a the phonetic transcription is needed at all.
</para>

<para>
  <screenshot>
    <screeninfo>Import dictionary: Introduction</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="import_dictionary_0.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>Since version 0.3 you can also import dictionaries directly to the active dictionary. This option is mostly there to make it easier to move to simon from custom solutions and to encourage importing of older models (for example one used with simon 0.2). You will <emphasis>almost never</emphasis> want to import a very large dictionary as active dictionary.</para>

<para>You can find a list of available dictionaries that work with simon on the <ulink url="http://www.simon-listens.org/wiki/index.php/English:_Shadow_dictionary">simon wiki</ulink>.</para>

<para>
simon is able to import five different types of dictionaries:
<itemizedlist>
  <listitem><para>HADIFIX</para></listitem>
  <listitem><para>HTK</para></listitem>
  <listitem><para>PLS</para></listitem>
  <listitem><para>SPHINX</para></listitem>
  <listitem><para>Julius</para></listitem>
</itemizedlist>
</para>



<sect3 id="import_dict_hadifix">
<title>HADIFIX Dictionary</title>
<para>
simon can import HADIFIX dictionaries.
</para>
<para>
One example of a HADIFIX dictionary is the German <ulink url="http://www.sk.uni-bonn.de/forschung/phonetik/sprachsynthese/bomp">HADIFIX BOMP</ulink>.
</para>
<para>
Hadifix dictionaries provide both terminals and pronunciation.
</para>

<para>
Due to a special exemption in their license the simon listens team is proud to be able to offer you to download the excellent HADIFIX BOMP directly from within simon.
</para>


<para>
  <screenshot>
    <screeninfo>Import dictionary: Automatic BOMP import</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="import_dictionary_hadifix_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>Using the automatic bomp import you can, after providing name and e-Mail address for the team of the University Bonn, directly download and import the dictionary from the simon listens server.</para>
</sect3>

<sect3 id="import_dict_htk">
<title>HTK Dictionary</title>
<para>
simon can import HTK lexica.
</para>
<para>
One example of a HTK lexicon is the English <ulink url="http://www.repository.voxforge1.org/downloads/SpeechCorpus/Trunk/Lexicon/">Voxforge dictionary</ulink>.
</para>
<para>
Hadifix dictionaries provide pronunciation information but no terminals. All words will be assigned to the terminal "Unknown".
</para>
</sect3>

<sect3 id="import_dict_pls">
<title>PLS Dictionary</title>
<para>
simon can import PLS dictionaries.
</para>
<para>
One example of a PLS dictionary is the <ulink url="http://www.repository.voxforge1.org/downloads/de/Trunk/Lexicon/">German GPL dictionary from Voxforge</ulink>.
</para>
<para>
PLS dictionaries provide pronunciation information but no terminals. All words will be assigned to the terminal "Unknown".
</para>
</sect3>

<sect3 id="import_dict_sphinx">
<title>SPHINX Dictionary</title>
<para>
simon can import SPHINX dictionaries.
</para>
<para>
One example of a SPHINX dictionary is this <ulink url="http://speech.mty.itesm.mx/~jnolazco/proyectos.htm">dictionary for Mexican Spanish</ulink>.
</para>
<para>
SPHINX dictionaries provide pronunciation information but no terminals. All words will be assigned to the terminal "Unknown".
</para>
</sect3>

<sect3 id="import_dict_julius">
<title>Julius Dictionary</title>
<para>
simon can import Julius vocabularies.
</para>
<para>
One example of a Julius vocabularies are the word lists of simon 0.2.
</para>
<para>
Julius dictionaries provide pronunciation information as well as terminal information.
</para>
</sect3>
</sect2> <!-- End Import Dict -->

</sect1> <!-- End Vocabulary -->


<sect1 id="grammar">
<title>Grammar</title>
<para>
The grammar defines which combinations of words are correct.
</para>

<sect2 id="grammar_general">
<title>General</title>
<para>
Let's look at an example: You want to use simon to launch programs and close those windows when you are done. You would like to use the following commands:
<itemizedlist>
  <listitem><para>"Computer, Internet" to open a browser</para></listitem>
  <listitem><para>"Computer, Mail"</para><para>To open a mail client</para></listitem>
  <listitem><para>"Computer, close"</para><para>To close the current window</para></listitem>
</itemizedlist>
</para>
<para>
Following English grammar, your vocabulary would contain the following:

<table frame='all'><title>Sample Vocabulary</title>
<tgroup cols='2' align='left' colsep='1' rowsep='1'>
<colspec colname='c1'/>
<colspec colname='c2'/>
<thead>
<row>
  <entry>Word</entry>
  <entry>Terminal</entry>
</row>
</thead>
<tbody>
<row>
  <entry>Computer</entry>
  <entry>Noun</entry>
</row>
<row>
  <entry>Internet</entry>
  <entry>Noun</entry>
</row>
<row>
  <entry>Mail</entry>
  <entry>Noun</entry>
</row>
<row>
  <entry>close</entry>
  <entry>Verb</entry>
</row>
</tbody>
</tgroup>
</table>
</para>

<para>
To allow the sentences defined above simon would need the following grammar:
<itemizedlist>
  <listitem><para>"Noun Noun" for sentences like "Computer Internet"</para></listitem>
  <listitem><para>"Noun Verb" for sentences like "Computer close"</para></listitem>
</itemizedlist>
</para>
<para>
While this would work, it would also allow the combinations "Computer Computer", "Internet Computer", "Internet Internet", etc. which are obviously bogus.
To improve the recognition accuracy, we can try to create a grammar that better reflects what we are trying to do with simon.
</para>

<para>
It is important to remember that you define your own "language" when using simon. That means that you are not bound to grammar rules that exist in whatever language you want to use simon with. For a simple command and control use-case it would for example be advisable to invent new grammatical rules to eliminate the differences between different commands imposed by grammatical information not relevant for this use case.
</para>

<para>
In the example above it is for example not relevant that "close" is a verb or that "Computer" and "Internet" are nouns. Instead, why not define them as something that better reflects what we want them to be:

<table frame='all'><title>Improved Sample Vocabulary</title>
<tgroup cols='2' align='left' colsep='1' rowsep='1'>
<colspec colname='c1'/>
<colspec colname='c2'/>
<thead>
<row>
  <entry>Word</entry>
  <entry>Terminal</entry>
</row>
</thead>
<tbody>
<row>
  <entry>Computer</entry>
  <entry>Trigger</entry>
</row>
<row>
  <entry>Internet</entry>
  <entry>Command</entry>
</row>
<row>
  <entry>Mail</entry>
  <entry>Command</entry>
</row>
<row>
  <entry>close</entry>
  <entry>Command</entry>
</row>
</tbody>
</tgroup>
</table>
</para>

<para>
Now we change the grammar to the following:
<itemizedlist>
  <listitem><para>"Trigger Command"</para></listitem>
</itemizedlist>

This allows all the combinations described above. However, it also limits the possibilities to exactly those three sentences. Especially in larger models a well thought grammar and vocabulary can mean a huge difference in recognition results.
</para>
</sect2>

<sect2>
<title>Defining your Grammar</title>
<para> 
simon provides an easy to use text based interface to change the grammar. You can simply list all the allowed sentences (without any punctuation marks, obviously) like described above.

<screenshot>
<screeninfo>Grammar</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="grammar.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>When selecting a sentence on the left, the right pane will automatically show possible real sentences with the words of your vocabulary on the right.</para>

<para>The example section will list at most 35 examples so if more than that amount of sentences match the selected grammar entry, the list might not be complete.</para>

</sect2>

<sect2>
<title>Import a Grammar</title>
<para>Additionally to simply entering your desired grammar sentence by sentence, simon is able to automatically deduce allowed grammar structures by reading plain text using the Import Grammar wizard.

<screenshot>
<screeninfo>Import Grammar</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="import_grammar.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>simon can read and import text files but also provides an input field if you want to simply type the text into simon.</para>

<para>Say we have a vocabulary like in the general section above: 

<table frame='all'><title>Improved Sample Vocabulary</title>
<tgroup cols='2' align='left' colsep='1' rowsep='1'>
<colspec colname='c1'/>
<colspec colname='c2'/>
<thead>
<row>
  <entry>Word</entry>
  <entry>Terminal</entry>
</row>
</thead>
<tbody>
<row>
  <entry>Computer</entry>
  <entry>Trigger</entry>
</row>
<row>
  <entry>Internet</entry>
  <entry>Command</entry>
</row>
<row>
  <entry>Mail</entry>
  <entry>Command</entry>
</row>
<row>
  <entry>close</entry>
  <entry>Command</entry>
</row>
</tbody>
</tgroup>
</table></para>

<para>We want simon to recognize the sentence "Computer Internet!". So we either enter the text using the <guibutton>Import text</guibutton> option or create a simple text file with this content "Computer Internet!" (any punctuation mark would work) and save it as "simongrammar.txt" to use the <guibutton>Import files</guibutton> option.

<screenshot>
<screeninfo>Import Grammar: Enter text</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="import_grammar1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

<screenshot>
<screeninfo>Import Grammar: Text file</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="import_grammar2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

<screenshot>
<screeninfo>Import Grammar: Select files</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="import_grammar3.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>simon will then read the entered text or all the given text files (in this case the only given text file is "simongrammar.txt") and look up every single word in both active and shadow dictionary (the definition in the active dictionary has more importance if the word is available in both). It will then replace the word with its terminal.</para>
<para>In our example this would mean that he would find the sentence "Computer Internet". simon would find out that "Computer" is of the terminal "Trigger" and "Internet" of the terminal "Command". Because of this simon would "learn" that "Trigger Command" is a valid sentence and add it to its grammar.</para>
<para>The import automatically segments the input text by punctuation marks (".", " - ", "!", etc.) so any natural text should work. The importer will automatically merge duplicate sentence structures (even across different files) and add multiple sentence (all possible combinations) when a word has multiple terminals assigned to it.</para>
<para>The import will ignore sentences where one or more words could not be found in the language model unless you tick the "Also import unknown sentences" checkbox in which case those words are replaced with "Unknown".</para> 

</sect2>





<sect2>
<title>Renaming Terminals</title>
<para>The rename terminal wizard allows you to rename terminals in both your active vocabulary, your shadow dictionary and the grammar.

<screenshot>
<screeninfo>Rename Terminal</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="rename_terminal.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect2>

<sect2>
<title>Merging Terminals</title>
<para>The merge terminal wizard allows you to merge two terminals into one new terminal in both your active vocabulary, your shadow dictionary and the grammar.

<screenshot>
<screeninfo>Merge Terminal</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="merge_terminal.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
<para>This functionality is especially useful if you want to simplify your grammar structures.</para>
</sect2>


</sect1>



<sect1 id="training">
<title>Training</title>

<para>Using the Training-module, you can improve your <link linkend="acoustic_model">acoustic model</link>. </para>

<para>
The interface lists all installed trainings-texts in a table consisting of three columns:
<itemizedlist>
<listitem><para>Name</para><para>A descriptive name for the text.</para></listitem>
<listitem><para>Pages</para><para>The number of "pages" the text consists of. Each page represents one recording.</para></listitem>
<listitem><para>Recognition Rate</para><para>Analogue to the vocabulary; Represents how likely simon will recognize the words (higher is better). The recognition rate of the trainings-text is the average recognition rate of all the words in the text.</para></listitem>
</itemizedlist>

<screenshot>
<screeninfo>Training</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="training.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
To improve the acoustic model - and thus the recognition rate - you have to record trainings-texts. This means that simon gets essentially two needed parts: 

<itemizedlist>
  <listitem><para>Samples of your speech</para></listitem>
  <listitem><para>Transcriptions of those samples</para></listitem>
</itemizedlist>

The active dictionary is used to transcribe the words (mapping them from the actual word to its phonetic transcription) that make up the text so every word contained in the trainings-text you want to read (train) has to be contained in your active dictionary. simon will warn you if this is not the case and provide you with the possibility to add all the missing words in one go.

<screenshot>
<screeninfo>Training: Warning about missing words</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="training_warning.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

The procedure is the same as if you would <link linkend="add_word">add a single word</link> but the wizard will prompt you for details and recordings for all the missing words automatically. This procedure can be aborted at any time and simon will provide both a way to add the already completely defined words and to undo all changes done so far. When the user has added all the words he is prompted for (all the words missing) the changes to the active dictionary / vocabulary are saved and the training of the previously selected text starts automatically.
</para>

<para>
The training (reading) of the trainings-text works exactly the same as the initial training when adding a new word.</para>

<para>Make sure you follow the guidelines listed in the <link linkend="recording">recording section</link>.</para>
<para>
<screenshot>
<screeninfo>Training in progress</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="training_running.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>



<sect2 id="training_storage">
<title>Storage Directories</title>
<para>
Trainings-texts are stored at two different locations:
<itemizedlist>
  <listitem><para>Linux: <filename>~/.kde/share/apps/simon/texts</filename></para>
            <para>Windows: <filename>%appdata%\.kde\share\apps\simon\texts</filename></para>
          <para>The texts of the current user. Can be deleted and added with simon (see below).</para></listitem>
  <listitem><para>Linux: <filename>`kde4-config --prefix`/share/apps/simon/texts</filename></para>
            <para>Windows: <filename>(install directory)\share\apps\simon\texts</filename></para>
          <para>System wide texts. They will appear on every user account using simon on this machine and can not be deleted from within simon because of the obvious permission restrictions on system wide files.</para>
          <para>This folder can be used by system administrators to provide a common set of trainings-texts for all the users on one system.</para>
          </listitem>
</itemizedlist>
</para>
<para>The XML files (one for each text) can just be moved from one location to the other but this will most likely require admin privileges.</para>
</sect2>


<sect2 id="add_texts">
<title>Adding Texts</title>
<para>
<screenshot>
<screeninfo>Import-trainings-texts-wizard</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="import_text_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

The add texts wizard provides a simple way to add new trainings-texts to simon.
</para>

<para>When importing text files, simon will automatically try to recognize individual sentences and split the text into appropriate "pages" (recordings). The algorithm treats text between "normal" punctuation (".", "!", "?", "...", """,...) and line breaks as "sentences". Each "sentence" will be on its own page.</para>

<para>
  simon supports two different sources for new trainings-texts.
</para>

<sect3>
  <title>Add trainings-texts</title>
            <para>
      <screenshot>
      <screeninfo>Import-trainings-texts-wizard: Add trainings-texts</screeninfo>
        <mediaobject>
          <imageobject>
            <imagedata fileref="import_text_4.png" format="PNG"/>
          </imageobject>
        </mediaobject>
      </screenshot>

        Simply enter the trainingstext in an input field.</para>
</sect3>
<sect3>
  <title>Local text files</title>

    <para>
      <screenshot>
      <screeninfo>Import-trainings-texts-wizard: Local text files</screeninfo>
        <mediaobject>
          <imageobject>
            <imagedata fileref="import_text_3.png" format="PNG"/>
          </imageobject>
        </mediaobject>
      </screenshot>

    simon can import normal text files to use them as trainings-texts.</para>
</sect3>

</sect2>

<sect2 id="special_training">
<title>On The Fly Training</title>
<para>Additionally to trainings-texts, simon also allows to train individual words or word combinations from your dictionary on-the-fly.</para>

<para>This feature is located in the vocabulary-menu of simon.</para>

<para>
<screenshot>
<screeninfo>Special Training: Selecting the Words</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="special_training.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>Select the words to train from the vocabulary on the left and simply drag them to the selection list to the right (you could also select them in the table on the left and add them by clicking "Add to Training".</para>

<para>Start the training by selecting "Train selected words". The training itself is exactly the same as if it were a pre-composed trainings-text.</para>

<para>
<screenshot>
<screeninfo>Special Training: Training the Words</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="special_training_2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>If there are more than 9 words to train simon will automatically split the text evenly across multiple pages.</para>

<para>Of course you are free to add words from the shadow lexicon to the list of words to train but simon will prompt you to add the words before the training starts just like he would if you would train a text that contains unknown words (see above).</para>
</sect2>

<sect2 id="import_trainings-data">
<title>Importing Trainings Samples</title>
<para> 
Using the import trainings-data field one can import previously gathered trainings-samples from previous simon versions or manual trainings without copying the whole dictionary.</para>

<para>
This feature is very specific. Please use it with caution and make sure that you know exactly what you are doing before you continue.
</para>

<para>You can either provide a separate prompts file or let simon extract the transcriptions from the filenames.</para>

<para>When using prompts based transcriptions your prompts file (UTF-8) needs to contain lines of the following content: "[filename] [content]". Filenames are without file extensions and the content has to be uppercase. For example: "demo_2007_03_20 DEMO" to import the file "demo_2007_03_20.wav" containing the spoken word "Demo".</para>

<para>Because prompts files do not contain a file extension, simon will try wav, mp3, ogg and flac (in that order). If one of those match, no other extension will be tested and only the first file will be imported (in contrast to file based transcription where all files would be imported).</para>

<para>
When using file based transcriptions, a file called this_is_a_test.wav <emphasis>must</emphasis> contain "This is a test" and nothing else. Numbers and special characters (".", "-",...) in the filename are ignored and stripped.
</para>

<para>Files recorded by simon 0.2 <emphasis>will</emphasis> follow this naming scheme so you can safely import them using the file name extraction method. Files generated by previous simon versions should not be imported using this function but you can use the prompts based import for that.</para>

<para>
Imported files and their transcription are then added to the trainings corpus.
</para>

<para>
To import a directory containing trainings-samples just select the folder to import and depending on your import type also the prompts file.

<screenshot>
<screeninfo>Import-trainings-data-wizard</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="import_training_data_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
The folder will be scanned recursively. This means that the given folder and all its subfolders will be searched for .wav, .flac, .mp3 and .ogg files. All files found will be imported.
</para>

<para>
When importing the sound files, all configured <link linkend="soundconfiguration_postprocessing">post processing filters</link> are applied.
</para>
<para>
  If you import anything other than WAV files you are responsible for decoding them during the import process (for example through post processing filters) or the model creation <emphasis>will</emphasis> fail.
</para>

</sect2>

</sect1>


<sect1 id="commands">
<title>Commands</title>
<para>When simon is active and recognizes something, the recognition result is given to the loaded command plug-ins (in order) for processing.</para>

<para>
<screenshot id="command_dialog">
<screeninfo>simons Command Dialog</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_main.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>The command system can be compared with a group of factory workers. Each one of them knows how to perform one task (e.g. "Karl" knows how to start a program and "Joe" knows how to open a folder, etc.). Whenever simon recognizes something it is given to "Karl" who then checks if this instruction is meant for him. If he doesn't know what to do with it, it is handed over to "Joe" and so on. If none of the loaded plugins know how to process the input it is ignored. The order in which the recognition result is given to the individual commands (people) is configurable in the command options (<guibutton>Commands</guibutton> > <guibutton>Manage plugins</guibutton>).</para>


<para>
<screenshot>
<screeninfo>simons Action Configuration</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_actions_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
Each plugin can be associated with a "trigger". Using triggers, the responsibility of each plugin can be easily be divided.
</para>
<para>Using the factory workers abstraction from above it could be compared to stating the name of who you mean to process your request. So instead of "Open my home folder" you say "Joe, open my home folder" and "Joe" (the plugin responsible for opening folders) will instantly know that the request is meant for him.</para>
<para>In practice you could have commands like the executable command "Firefox" to open the popular browser and the place command "Google" to open the web search engine. If you assign the trigger "Start" to the executable plugin and the trigger "Open" to the place command you would have to say "Start Firefox" (instead of just "Firefox" if you don't use a trigger for the executable plugin) and "Open Google" to open the search engine (instead of just "Google").</para>
<para>Triggers are of course no requirement and you can easily use simon without defining any plugin triggers (although many plugins come with a default trigger of "Computer" set which you would have to remove). But even if you use just on trigger for all your commands (like "Computer" to say "Computer, Firefox" and "Computer, Google" like) it has the advantage of greatly limiting the number false-positives.</para>

<para>simons <link linkend="command_dialog">command dialog</link> displays the complete phrase associated with a command in the upper right corner of the command configuration.</para>

<para>You can load multiple instances of one plugin even in one scenario. Each instance can of course also have a different plugin trigger.</para>

<para>
Each Command has a name (which will trigger its invocation), an icon and more fields depending on the type of the plugin (see below).
</para>

<para>
Some command plugins might provide a configuration of the plugin itself (not the commands it contains). There configuration pages will be plugged directly into the action configuration dialog (below the <guimenuitem>General</guimenuitem> menu item) when you load the associated plugin.
</para>

<para>
  Plugins that provide a graphical user interface (like for example the <link linkend="input_number">input number command plugin</link>) can be configured by configuring "Voice commands". You can change the associated word that will trigger the button, for example, but also change the displayed icon, etc. If you remove all voice interface commands from a graphical element, the element will be hidden automatically.
</para>

<para>Voice interface commands are added just like normal commands through the command configuration.</para>

<para>
<screenshot>
<screeninfo>Configure voice interface commands</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="voice_interface_command_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>To add a new interface command to a function, just select the action you want to associate with a command, click <guibutton>Create from template</guibutton> and adapt the resulting commando to your needs.</para>

<para>
  Some plugins (for example the <link linkend="desktopgrid">desktopgrid</link> or the <link linkend="calculator_command_plugin">calculator</link> might also provide a menu item in the <guimenu>Commands</guimenu> menu.
</para>

<para>
<screenshot>
<screeninfo>Command plugged into main window</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_plug.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<sect2 id="executable_commands">
<title>Executable Commands</title>

<para>Executable commands are associated with an executable file ("Program") which is started when the command is invoked.</para>

<screenshot>
<screeninfo>Executable Commands</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_program.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

<para>Arguments to the commands are supported. If either path to the executable or the parameters contain spaces they <emphasis>must</emphasis> be wrapped in quotes.</para>

<para>Given the executable file <filename>C:\Program Files\Mozilla Firefox\firefox.exe</filename> the local html file <filename>C:\test file.html</filename> the correct line for the "Executable" would be: <filename>"C:\Program Files\Mozilla Firefox\firefox.exe" "C:\test file.html"</filename>.</para>

<para>The working directory defines where the process should be launched from. Given the working directory <filename>C:\folder</filename>, the command <filename>"C:\Program Files\Mozilla Firefox\firefox.exe" file.html</filename> would cause firefox to search for the file <filename>C:\folder\file.html</filename>.</para>

<para>The working directory does not normally need to be set and can be left blank most of the time.</para>

<sect3 id="importing_executable_commands">
<title>Importing Programs</title>
<para>For even easier configuration simon provides an import dialog which allows you to select programs directly from the KDE menu.</para>

<note><para>This option is not available on Microsoft Windows.</para></note>

<screenshot>
<screeninfo>Import Programs</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_program_import_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

<para>The dialog will list all programs that have an entry in your KDE menu in their respective category.</para>

<para>Sub-Categories are not supported and are thus listed on the same level as top-level categories.</para>

<para>Just select the program you wish to start with simon and press <guibutton>Ok</guibutton>. The correct values for the executable and the working directory as well as an appropriate command name and description will automatically be filled out for you.</para>

</sect3>

</sect2>

<sect2 id="place_commands">
<title>Place Commands</title>
<para>With place commands you can allow simon to open any given URL. Because simon just hands the address over to the platforms URL handler, special Protocols like "remote:/" (on Linux/KDE) or even KDEs "Web-Shortcuts" are supported.</para>
<para>Instead of folders, files can also be set as the commands URL which will cause the file to be opened with the application which is associated with it when the command is invoked.</para>

<screenshot>
<screeninfo>Places</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_place.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

<para>To associate a specific URL with the command you can manually enter it in the URL field (select <guibutton>Manual</guibutton> first) or import it with the import place wizard.</para>


<sect3 id="importing_place_commands">
<title>Importing Places</title>

<para>The import place dialog allows you to easily create the correct URL for the command.</para>

<para>To add a local folder, select <guibutton>Local Place</guibutton> and choose the folder or file with the file selector.

<screenshot>
<screeninfo>Import Places: Local</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_place_import_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>To add a remote URL (HTTP, FTP, etc.) choose <guibutton>Remote URL</guibutton>

<screenshot>
<screeninfo>Import Places: Remote</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_place_import_2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

Please note that for URLs with authentication information the password will be stored in clear text.</para>

</sect3>
</sect2>

<sect2 id="shortcut_commands">
<title>Shortcut Commands</title>
<para>Using shortcut commands the user can associate commands with key-combinations.</para>

<para>The command will simulate keyboard input to "press" shortcuts like "Ctrl+C" or "Alt+F4".

<screenshot>
<screeninfo>Defining Shortcut Commands</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_shortcut.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>To select the shortcut you wish to simulate just toggle the shortcut button and press the key combination on your keyboard.</para>
<para>simon will capture the shortcut and associate it with the command.</para>
<para>Due to technical limitations there are several shortcuts on Microsoft Windows that can not be captured by simon (this includes e.g. Ctrl+Alt+Del and Alt+F4). These special shortcuts can be selected from a list below the aforementioned shortcut button.</para>

<note><para>This selection box is not visible in the screenshot above as the list is only displayed in the Microsoft Windows version of simon.</para></note>
</sect2>

<sect2 id="text_macro_commands">
<title>Text-Macro Commands</title>
<para>Using text-macro commands, the user can associate text with a command. When the command is invoked, the associated text will be "written" by simulating keystrokes.</para>

<para>
<screenshot>
<screeninfo>Defining Text-Macro Commands</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_text-macro.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect2>

<sect2 id="list_command">
<title>List Commands</title>
<para>The list command is designed to combine multiple commands (all types of commands are supported) into one list. The user can then select the n-th entry by saying the associated number (1-9).</para>

<para>This is very useful to limit the amount of training required and provides the possibility to keep the vocabulary to a minimum.</para>

<para>
<screenshot>
<screeninfo>Defining List Commands</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_list_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>List commands are especially useful when using commands with difficult triggers or commands that can be grouped under a general theme. A typical example would be a command "Startmenu" to present a list of programs to launch. That way the specific executable commands can still retain very descriptive names (like "OpenOffice.org Writer 3.1") without the user having to include these words in his vocabulary and consider them in the grammar just to trigger them.</para>

<para>Commands of different types can of course be mixed.</para>

<sect3 id="list_command_display">
<title>List Command Display</title>

<para>
When invoked, the command will display the list centered on the screen. The list will automatically expand to accompany its items.
<screenshot>
<screeninfo>Defining List Commands</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_list_2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
The user can invoke the commands contained in the list by simply saying their associated number (In this example: "One" to launch Mozilla Firefox).
</para>

<para>
While a list command is active (displayed), all input that is not directed at the list itself (other commands, etc.) will be rejected. The process can be canceled by pressing the "Cancel" button or by saying "Cancel".</para>

<para>
If there are more than 9 items simon will add "Next" and "Back" options to the list ("Zero" will be associated with "Back" and "Nine" with "Next").

<screenshot>
<screeninfo>List Command with many entries</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_list_3.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect3>

<sect3 id="list_command_configure">
<title>Configuring list elements</title>

<para>
By default the list command uses the following trigger words. To use list commands to their full potential, make sure that your language and acoustic model contains and allows for the following "sentences":
<itemizedlist>
<listitem><para>"Zero"</para></listitem>
<listitem><para>"One"</para></listitem>
<listitem><para>"Two"</para></listitem>
<listitem><para>"Three"</para></listitem>
<listitem><para>"Four"</para></listitem>
<listitem><para>"Five"</para></listitem>
<listitem><para>"Six"</para></listitem>
<listitem><para>"Seven"</para></listitem>
<listitem><para>"Eight"</para></listitem>
<listitem><para>"Nine"</para></listitem>
<listitem><para>"Cancel"</para></listitem>
</itemizedlist>
</para>

<para>Of course you can also configure these words in your simon configuration:
  <itemizedlist>
    <listitem>
      <para><guimenuitem>Commands</guimenuitem> > <guibutton>Manage plugins</guibutton> > <guimenuitem>General</guimenuitem> > <guimenuitem>Lists</guimenuitem> for the scenario wide list configuration.</para>
    </listitem>
    <listitem>
      <para><guimenu>Settings</guimenu> > <guimenuitem>Configure simon...</guimenuitem> > <guimenuitem>Actions</guimenuitem> > <guimenuitem>Lists</guimenuitem> for the global configuration. When creating a new scenario, the scenario configuration will be initialized with a copy of this list configuration.</para>
    </listitem>
  </itemizedlist>
</para>

<para>
List commands are internally also used by other plugins like for example the <link linkend="desktopgrid">desktopgrid</link>. The confiugration of the triggers also affects their displayed lists.
</para>

</sect3>

</sect2>

<sect2 id="composite_commands">
<title>Composite Commands</title>
<para>Composite commands allow the user to group multiple commands into a sequence.</para>

<para>When invoked the commands will be executed in order. Delays between commands can be inserted.

<screenshot>
<screeninfo>Defining Composite Commands</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_composite_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
Using the composite command the user can compose complex "macros". The screenshot above - for example - does the following:
<itemizedlist>
<listitem><para>Start Kopete (Executable Command)</para></listitem>
<listitem><para>Wait 2000ms for Kopete do be started</para></listitem>
<listitem><para>Type "Mathias" (Text-Macro Command) which will select Mathias in my contact list</para></listitem>
<listitem><para>Press Enter (Shortcut Command)</para></listitem>
<listitem><para>Wait 1000ms for the chat window to appear</para></listitem>
<listitem><para>Write "Hi!" (Text-Macro Command); The text associated to this command contains a newline at the end so that the message will be send.</para></listitem>
<listitem><para>Press Alt+F4 (Shortcut Command) to close the chat window</para></listitem>
<listitem><para>Press Alt+F4 (Shortcut Command) to close the kopete main window</para></listitem>
</itemizedlist>
</para>

</sect2>

<sect2 id="desktopgrid">
<title>Desktopgrid</title>
<para>The desktopgrid allows the user to control his mouse with his voice.</para>

<para>
<screenshot>
<screeninfo>The Desktopgrid</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_desktopgrid.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
The desktopgrid divides the screen into nine parts which are numbered from 1-9. Saying one of these numbers will again divide the selected field into 9 fields again numbered from 1-9, etc. This is repeated 3 times. After the fourth time the desktopgrid will be closed and simon will click in the middle of the selected area.</para>

<para>The exact click action is configurable but defaults to asking the user. Therefore you will be presented with a list of possible click modes. When selecting Drag and Drop, the desktopgrid will be displayed again to select the drop point.</para>

<para>
<screenshot>
<screeninfo>Desktopgrid: Click selection</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="desktop_grid_click_selection.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
While the desktopgrid is active (displayed), all input that is not directed at the desktopgrid itself (other commands, etc.) will be rejected. Say "Cancel" at any time to abort the process.
</para>

<para>The desktopgrid plugin registers a configuration screen right in the command configuration when it is loaded.</para>

<para>
<screenshot>
<screeninfo>Configuring the Desktopgrid</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="desktopgrid_configure.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
The trigger that invokes the desktopgrid is of course completely configurable. Moreover the user can use "real" or "fake" transparency. If your graphical environment allows for compositing effects ("desktop effects") then you can safely use "real" transparency which will make the desktogrid transparent. If your platform does not support compositing simon will simulate transparency by taking a screenshot of the screen before displaying the desktopgrid and display that picture behind the desktopgrid.
</para>

<para>If the desktopgrid is configured to use real transparency and the system does not support compositing it will display a solid gray background.</para>

<para>
However, nearly all up-to-date systems will support compositing (real transparency).
</para>
<para>This includes:
<itemizedlist>
<listitem><para>Microsoft Windows 2000 or higher (XP, Vista, 7)</para></listitem>
<listitem><para>GNU/Linux using a composite manager like Compiz, KWin4, xcompmgr, etc.</para></listitem>
</itemizedlist>
</para>


<para>
By default the desktopgrid uses numbers to select the individual fields. To use the desktopgrid, make sure that your language and acoustic model contains and allows for the following "sentences":
<itemizedlist>
<listitem><para>"One"</para></listitem>
<listitem><para>"Two"</para></listitem>
<listitem><para>"Three"</para></listitem>
<listitem><para>"Four"</para></listitem>
<listitem><para>"Five"</para></listitem>
<listitem><para>"Six"</para></listitem>
<listitem><para>"Seven"</para></listitem>
<listitem><para>"Eight"</para></listitem>
<listitem><para>"Nine"</para></listitem>
<listitem><para>"Cancel"</para></listitem>
</itemizedlist>
</para>

<para>To configure these triggers, just configure the commands associated with the plugin.</para>

<para>
<screenshot>
<screeninfo>Desktopgrid: Configuring list elements</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="desktop_grid_configure_list_elements.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

</sect2>

<sect2 id="input_number">
<title>Input Number</title>
<para>Using the input-number plugin the user can input large numbers easily.</para>


<para>Using the Dictation or the Text-Macro plugin one could associate the numbers with their digits and use that as input method. however, to input larger numbers there are two ways that both have significant disadvantages:

<itemizedlist>
<listitem><para><emphasis>Adding the words "eleven, "twelve", etc.</emphasis></para>
<para>While this seems like the most elegant solution as it would enable the user to say "fivehundredseventytwo" we can easily see that it would be quite a problem to add all these words - let alone train them. What about "twothousandninehundredtwo"? Where to stop?</para></listitem>
<listitem><para><emphasis>Spell out the number using the individual digits</emphasis></para>
<para>While this is not as elegant as stating the complete number it is much more practical.</para>
<para>However, many applications (like the great mouseless browsing firefox addon) rely on the user to input large numbers without too much time passing between the individual keystrokes (mouselss browsing for example will wait exactly 500ms per default before it considers the input of the number complete). So if you want to enter 52 you would first say "Five (pause) Two". Because of the needed pause, the application (like the mouseless browsing plugin) would consider the input of "Five" complete.</para></listitem>
</itemizedlist>
</para>

<para>
The input number plugin - when triggered - presents a calculator-like interface for inputting a number. The input can be corrected by saying "Back". It features a decimal point accessible by saying "Comma". When saying "Ok" the number will be typed out. As all the voice-input and the correction is handled by the plugin itself the application that finally receive the input will only get couple of milliseconds between the individual digits.

<screenshot>
<screeninfo>Input Number Plugin</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_input_number.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
While the input number plugin is active (the user currently inputs a number), all input that is not directed at the input number plugin (other commands, etc.) will be rejected. Say "Cancel" at any time to abort the process.
</para>

<para>As there can no command instances be created of this plugin it is not listed in the "New Command" dialog. However, the input number plugin registers a configuration screen right in the command configuration when it is loaded.

<screenshot>
<screeninfo>Input Number Plugin</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="input_number_configure.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

The trigger defines what word or phrase that will trigger the display of the interface.
</para>


<para>
By default the input number plugin uses numbers to select the individual digits and a couple of control words. To use the input number plugin, make sure that your language and acoustic model contains and allows for the following "sentences":
<itemizedlist>
<listitem><para>"Zero"</para></listitem>
<listitem><para>"One"</para></listitem>
<listitem><para>"Two"</para></listitem>
<listitem><para>"Three"</para></listitem>
<listitem><para>"Four"</para></listitem>
<listitem><para>"Five"</para></listitem>
<listitem><para>"Six"</para></listitem>
<listitem><para>"Seven"</para></listitem>
<listitem><para>"Eight"</para></listitem>
<listitem><para>"Nine"</para></listitem>
<listitem><para>"Back"</para></listitem>
<listitem><para>"Comma"</para></listitem>
<listitem><para>"Ok"</para></listitem>
<listitem><para>"Cancel"</para></listitem>
</itemizedlist>
</para>

<para>To configure these triggers, just configure the commands associated with the plugin.</para>

<para>
<screenshot>
<screeninfo>Input Number Plugin</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_number_input_elements.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

</sect2>

<sect2 id="dictation_command_plugin">
<title>Dictation</title>
<para>The dictation plugin writes the recognition result it gets using simulated keystrokes.</para>

<para>Assuming you didn't define a trigger for the dictation plugin it will accept all recognition results and just write them out. The written input will be considered as "processed input" and thus not be relayed to other plugins. This means that if you loaded the dictation plugin and defined no trigger for it, all plugins <emphasis>below it</emphasis> in the "Selected Plug-Ins" list in the command configuration will never receive any input.</para>

<para>As there can no command instances be created of this plugin it is not listed in the "New Command" dialog.</para>

<para>The dictation plugin can be configured to append texts after recognition results to for example add a space after each recognized word.</para>

<para>
<screenshot>
<screeninfo>Configure dictation</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_dictation.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect2>

<sect2 id="ai_command_plugin">
<title>Artificial Intelligence</title>
<para>The Artificial Intelligence is a just-for-fun plugin that emulates a human conversation.</para>

<para>Using the festival text to speech technology the computer can "talk" with the user and answer question or chat about the weather.</para>

<para>The plugin uses AIMLs for the actual "intelligence". Most AIML sets should be supported. The popular <ulink url="http://www.pandorabots.com/pandora/talk?botid=f5d922d97e345aa1">A. L. I. C. E. bot</ulink> and a German version work and are shipped with the plugin.

<screenshot>
<screeninfo>AI Plugin</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="ai_configure.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

The plugin registers a configuration screen in the command configuration menu where you can choose which AIML set to load.
</para>

<para>simon will look for AIML sets in the following directory: 
<itemizedlist>
<listitem><para>GNU/Linux: <filename>`kde4-config --prefix`/share/apps/ai/aimls/</filename></para></listitem> 
<listitem><para>Microsoft Windows: <filename>[installation folder (C:\Program Files\simon 0.2\ by default)]\share\apps\ai\aimls\</filename></para></listitem>
</itemizedlist>
To add a new set just create a new directory with a descriptive name and copy the .aiml files into it.
</para>

<para>
To adjust your bots personality have a look at the bot.xml and vars.xml files in the following directory:
<itemizedlist>
<listitem><para>GNU/Linux: <filename>`kde4-config --prefix`/share/apps/ai/util/</filename></para></listitem> 
<listitem><para>Microsoft Windows: <filename>[installation folder (C:\Program Files\simon 0.2\ by default)]\share\apps\ai\util\</filename></para></listitem>
</itemizedlist>
</para>

<para>The plugin will use <ulink url="http://tcts.fpms.ac.be/synthesis/mbrola.html">mbrola voices</ulink> if they are installed.</para>

<para>As there can no command instances be created of this plugin it is not listed in the "New Command" dialog.</para>

<para>It is recommended to not use any trigger for this plugin to provide a more natural "feel" for the conversation. The AI plugin <emphasis>will</emphasis> pass any input through to the other plugins, even tough it will react on any input given. This makes it possible to add a "conversation" to the command &amp; control use-case by developing custom AIMLs sets (e.g.: User: "Computer, open Firefox"; Computer: "Certainly, Sir! Starting Firefox..."; Firefox opens).</para>

<para>Please keep in mind that the AI plugin will only work if festival is installed, set-up correctly and lies in your system path.</para>
</sect2>

<sect2 id="calculator_command_plugin">
<title>Calculator</title>
<para>The calculator plugin is a simple, voice controlled calculator.</para>

<para>
<screenshot>
<screeninfo>Calculator plugin</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="calculator.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>The calculator extends the <link linkend="input_number">Input Number</link> plugin by providing additional features.</para>

<para>When loading the plugin, a configuration screen is added to the plugin confiugration.</para>

<para>
<screenshot>
<screeninfo>Calculator plugin: Configuration</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="calculator_configuration.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>There you can also confiugre the control mode of the calculator. Setting the mode to something else than <guimenuitem>Full calculator</guimenuitem> will hide options from the displayed widget.</para>

<para>
<screenshot>
<screeninfo>Calculator plugin: Minimal</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="calculator_minimal.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>However, the hidden controls will, in contrast to simply removing all associated command from the functions, still react to the configured voice commands.</para>

<para>When selecting <guibutton>Ok</guibutton>, the calculator will by default ask you what to do with the generated result. You can for example output the calculation, the result, both, etc. Besides always selecting this from the displayed list after selecting the <guibutton>Ok</guibutton> button, this can also be set in the configuration options.</para>

<para>
<screenshot>
<screeninfo>Calculator plugin: Output mode selection</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="calculator_output_mode_selection.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

</sect2>

<sect2 id="filter_command_plugin">
  <title>Filter</title>
  <para>Using the filter plugin, you can intercept recognition results from being passed on to further command plugins. Using this plugin you can for example disable the recognition by voice.</para>
  <para>The filter has two states:
    <itemizedlist>
      <listitem>
        <para>Inactive</para>
        <para>The default state. All recognition results will be passed through.</para>
      </listitem>
      <listitem>
        <para>Active</para>
        <para>When activated, the filter will "eat" all results that match the configured pattern. By default this means every result that simon recognizes will be accepted by the filter and therefore not relayed to any of the plugins following the filter plugin.</para>
      </listitem>
    </itemizedlist>
  </para>

  <para>
    The filter command plugin registers a configuration screen in the command configuration where you can change what results should be filtered.
  </para>
 
  <para>
  <screenshot>
  <screeninfo>Filter plugin: Configuration</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="filter_configuration.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>The pattern is a regular expression that will be evaluated each time a recognition results receives the plugin for processing.</para>

  <para>The plugin also registers voice interface commands for activating and deactivating the filter.</para>

</sect2>

<sect2 id="pronunciation_training_command_plugin">
  <title>Pronunciation Training</title>

  <para>The pronunciation training, when combined with a good <link linkend="base_model">static base model</link>, can be a powerful tool to improve your pronunciation of a new language.</para>
 
  <para>
  <screenshot>
  <screeninfo>Pronunciation training</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="pronunciation_training.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>Essentially, the plugin will prompt you to say specific words. The recognition will then recognize your pronunciation of the word and compare it to your speech model which should be a base model of native speakers for this to work correctly. Then simon will display the recognition rate (how similar your version was to the stored base model).</para>

  <para>The closer to the native speaker, the higher the score.</para>

  <para>The plugin adds an entry to your <guimenu>Commands</guimenu> menu to launch the pronunciation training dialog.</para>

  <para>The training itself consists of multiple pages. Each page contains one word fetched from your active vocabulary. They are identified by a terminal which needs to be selected in the command configuration before starting the training.</para>

  <para>
  <screenshot>
  <screeninfo>Pronunciation training: Confiugration</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="pronunciation_training_configuration.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>
</sect2>

<sect2 id="keyboard_command_plugin">
  <title>Keyboard</title>

  <para>The keyboard plugin displays a virtual, voice controlled keyboard.</para>

  <para>
  <screenshot>
  <screeninfo>Keyboard</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="keyboard_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>The keyboard consits of multiple tabs, each possibly containing many keys. The entirety of tabs and keys are collected in "sets".</para>

  <para>You can select sets in the configuration but also create new ones from scratch in the keyboard command confiugration.</para>

  <para>
  <screenshot>
  <screeninfo>Keyboard: Confiugration</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="keyboard_2.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>Keys are usually mapped to single characters but can also hold long texts and even shortcuts. Because of this, keyboard sets can contain special keys like a "select all" key or a "Password" key (typing your password).</para>

  <para>Next to the tabs that hold the keys of your set, the keyboard my also show special keys like Ctrl, Shift, etc. Those keys are provided as voice inteface commands and are displayed regardless of what tab of the set is currently active.</para>
  <para>As with all voice triggers, removing the associated command, hides the buttons as well.</para>

  <para>Moreover, the keyboard provides a numpad that can be shown by selecting the appropriate option in the keyboard configuration.</para>

  <para>
  <screenshot>
  <screeninfo>Keyboard: Keypad</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="keyboard_3.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>Next to the number keys and the delete key for the number input field (<guibutton>Number backspace</guibutton>), the numpad provides two options on what to do with the entered number.</para>
  <para>When selecting <guibutton>Write number</guibutton>, the entered number will be written out using simulated key presses. Selecting <guibutton>Select number</guibutton> tries to find a key or tab in the currently active set that has this number as a trigger. This way you can control a complete keyboard just using numbers.</para>

  <para>
  <screenshot>
  <screeninfo>Keyboard: Number based</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="keyboard_4.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>The keys on the num pad are configurable voice interface commands.</para>
</sect2>
</sect1>



<sect1 id="configuration">
<title>Configuration</title>
<para>
simon was designed with high configurability in mind.
</para>

<sect2 id="configuration_general">
<title>General Configuration</title>
<para>The general configuration page lists some basic settings.</para>
<para>
<screenshot>
<screeninfo>General Configuration</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_general.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
<para>
Please note that the option to start simon at login will work on both Microsoft Windows and when you are using KDE on Linux. Support for other desktop environments like Gnome, XFCE, etc. might require manually placing simon in the session autostart (please refer to the respective manuals of your desktop environment).
</para>
<para>
When the option to start simon minimized is selected, simon will minimize to the system tray immediately after starting.
</para>
<para>
  Deselecting the option to warn when there are problems with samples deactivates the <link linkend="sample_quality_assurance">sample quality assurance</link>.
</para>
</sect2>

<sect2 id="soundconfiguration">
<title>Sound Configuration</title>
<para>
simon uses QtMultimedia to record and play sound. QtMultimedia is also used to gather data from the microphone which is then sent to the simond server for recognition.
</para>

<sect3 id="soundconfiguration_device">
<title>Device Configuration</title>
<para>The sound device configuration allows you to choose which sound device(s) to use, how many channels to use and at which samplerate to record.</para>
<para>
Most of the time you will want to use 1 channel and 16kHz (which is also the default) because the recognition only works on mono input and works best at 16kHz (8kHz being the other option).</para>

<para>However, some low-cost sound cards might not support this particular mode in which case simon can in many cases work around this limitation by using postprocessing chains and 3rd party software. Please see the <link linkend="soundconfiguration_postprocessing">postprocessing section</link> for more details.</para>

<para>Bottom line: Only change the channel and the samplerate if you really know what you are doing. Otherwise the recognition will <emphasis>most likely</emphasis> not work.</para>

<para>
<screenshot>
<screeninfo>Sound Configuration: General</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_sound_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>Use the selection boxes to change the device. Use the <guibutton>Refresh devices</guibutton> button if you have changed the sound configuration since you started simon.</para>

<para>You can use simon with more than one sound device at the same time. Use <guibutton>Add device</guibutton> to add a new device to the configuration and <guibutton>Remove device</guibutton> to remove it from your configuration.</para>

<para>The first device in your sound setup can not be removed.</para>

<para>For each device you can determine for what you want the device to be used: Training or recognition (last one only applicable for input devices).</para>

<para>If you use more than one device for training, you will create multiple sound files for each utterance. When using multiple devices for recognition each one feeds a separate sound input stream to the server resulting in recognition results for each stream.</para>

<para>If you use multiple output devices the playback of the trainings samples will play on all configured audio devices.</para>

<para>When using different sample rates for your input devices, the output will only play on matching output devices. If you for example have one input device configured to use 16kHz and the other to use 48kHz, the playback of samples generated by the first one will only play on 16kHz outputs, the other one only on 48kHz devices.</para>
</sect3>

<sect3 id="soundconfiguration_vad">
  <title>Voice Activity Detection</title>

  <para>The recognition is done one the simond server. See the <link linkend="architecture">architecture section</link> for more details.</para>

  <para>The sound stream is not continuous but is segmented by the simon client. This is done by something called "voice activity detection".</para>

  <para>
  <screenshot>
  <screeninfo>Voice activity detection</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="vad.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>Here you can configure this segmentation through the following parameters:
    <itemizedlist>
      <listitem>
        <para><guilabel>Cutoff level</guilabel></para>
        <para>Everything below this level is considered "silence" (background noise).</para>
      </listitem>
      <listitem>
        <para><guilabel>Head margin</guilabel></para>
        <para>Cache for as long as head margin to start consider it a real sample. During this whole time the input level needs to be above the cutoff level.</para>
      </listitem>
      <listitem>
        <para><guilabel>Tail margin</guilabel></para>
        <para>After the recording went below the cutoff level, simon will wait for as long as tail margin to consider the current recording a finished sammple.</para>
      </listitem>
      <listitem>
        <para><guilabel>Skip samples shorter than</guilabel></para>
        <para>Samples that are shorter than this value are not considered for recognition. (coughs, etc.)</para>
      </listitem>
    </itemizedlist>
  </para>
</sect3>

<sect3 id="soundconfiguration_training">
  <title>Training settings</title>

  <para>
  <screenshot>
  <screeninfo>Training settings</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="training_config.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>When the option <guibutton>Default to power training</guibutton> is selected, simon will, when training, automatically start- and stop the recording when displaying and hiding (respectively) the recording prompt. This option only sets the default value of the option, the user can change it at any time before beginning a training session.</para>

<para>The confiugrable font here refers to the text that is recorded to train the acoustic model (through explicit training or when adding a word).</para>

<para>This option has been introduced after we have worked with a few clients suffering spastic disability. While we used the mouse to control simon during the training, they had to read what was on the screen. At first this was very problematic as the regular font size is relatively small and they had trouble making out what to read. This is why we made the font and the font size of the recording prompt configurable.</para>

<para>Here you can also define the required signal to noise ratio for simon to consider a training sample to be correct. See the <link linkend="sample_quality_assurance">Sample Quality Assurance</link> section for more details.</para>
<para>On this configuration page you can also set the parameters for the <link linkend="volume_calibration">volume calibration</link>.</para>

<para>It can be deactivated for both the add word dialog and the trainings wizard by unchecking the group box itself. As long as the volume is not louder than the minimum volume simon will prompt the user to raise the microphone volume. If the recording hits the maximum volume once, simon will tell the user to lower the volume.</para>
<para>Clipping (hitting the maximum amplitute) will always cause a "too loud" warning.</para>
<para>The prompted text can be configured by entering text in the input field below. If the edit is empty a default text will be used.</para>
</sect3>

<sect3 id="soundconfiguration_postprocessing">
<title>Postprocessing</title>
<para>All recorded (training) and imported (through the import training data) samples can be processed using a series of postprocessing commands. Postprocessing chains are an advanced feature and shouldn't be needed by the average user.</para>

<para>
<screenshot>
<screeninfo>Sound Configuration: Postprocessing</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_sound_4.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>The postprocessing commands can be seen as a chain of filters through which the recordings have to pass through. Using these "filters" one could define commands to suppress background noise in the training data or normalize the recordings.</para>

<para>Given the program "process_audio" which takes the input- and output files as its arguments (e.g.: "process_audio in.wav out.wav") the postprocessing command would be: "process_audio %1 %2". The two placeholders %1 and %2 will be replaced by the input filename and the output filename respectively.</para>

<para>The switch to "apply filters to recordings recorded with simon" enables the postprocessing chains for samples recorded during the training (including the initial training while adding the word). If you don't select this switch the postprocesing commands are only applied to imported samples (through the <link linkend="import_trainings-data">import trainings-data wizard</link>).</para>

<para>One common use-case of postprocessing chains would be the resampling of audio because of hardware limitations. Given a soundcard that does not support mono 16kHz recordings but only supports 44100Hz stereo ("CD") recordings, one could use the free command line sound processing utility <ulink url="http://sox.sourceforge.net">SoX</ulink> to resample the recorded files after the recording.</para>
<para>This example would require the following postprocessing command:
<itemizedlist>
<listitem><para><filename>sox -c 1 -r 16000 %1 %2</filename></para></listitem>
</itemizedlist>
</para>

<para>Using this command you can safely record in 44100Hz and 2 channels and - assuming the option to apply the filters to recordings recorded with simon is selected - simon will automatically downsample them to 16000Hz and 1 channel automatically after recording them. Make sure to adjust your <link linkend="soundconfiguration_device">sound device configuration</link> accordingly.</para>
</sect3>
</sect2>

<sect2 id="configuration_speechmodel">
<title>Speech Model</title>
<para>Here you can adjust the parameters of the speech model.</para>

<para>
<screenshot>
<screeninfo>Speech Model Configuration</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_speech_model.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
<para>The samplerate set here is the target samplerate of the acoustic model. It has nothing to do with the recording samplerate and it is the responsibility of the user to ensure that the samples are actually made available in that format (usually by recording in that exact samplerate or by defining postprocessing commands that resample the files; see the <link linkend="soundconfiguration">sound configuration section</link> for more details).</para>

<para>Usually either 16kHz or 8kHz models are built / used. 16kHz models will have higher accuracy over 8kHz models. Going higher than 16kHz is not recommended as it is very cpu-intensive and in practice probably wont result in higher recognition rates.</para>

<para>Moreover, the path to the trainings-samples can be adjusted. However, be sure that the previously gathered trainings-samples are also moved to the new location. If you use automatic synchronization the simond would alternatively also provide simon with the missing sample but copying them manually is still recommended for performance reasons.</para>

</sect2>

<!--
<sect2 id="configuration_model_extensions">
<title>Model Extensions</title>

<para>Using the model (internet) extensions simon can fetch additional resources around your speech model from the internet. At the moment only the download of trainings-texts is supported but more features are planned for the future.</para>
<para>
<screenshot>
<screeninfo>Model Extensions</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_model_extensions.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
<para>
simon provides the feature to <link linkend="import_texts_remote">download new texts directly off of the internet</link>. This is based on a central repository. The URL to this repository can be changed here. The default configuration points to a sample repository hosted by the simon team.
</para>

</sect2>
-->


<sect2 id="configure_model">
<title>Model Settings</title>

<sect3 id="configure_model_general">
  <title>General</title>
<para>Please see the <link linkend="base_model_use">base model section</link>.</para>
</sect3>

<sect3 id="configure_model_extensions">
<title>Extensions</title>
<para>Here you can configure the base URL that is going to be used for the <link linkend="import_dict_hadifix">automatic bomp import</link>. The default points to the copy on the simon listens server.</para>
</sect3>
</sect2>

<sect2 id="configure_recognition">
<title>Recognition</title>

<para>Here you can configure the recognition and model synchronization with the simond server.</para>

<sect3 id="configure_server">
  <title>Server</title>

  <para>Using the server configuration you can set parameters of the connection to simond.</para>

<sect4 id="configure_server_general">
<title>General</title>
<para>
The simon main application connects to the simond server (see the <link linkend="architecture">architecture section</link> for more information).
</para>

<para>
<screenshot>
<screeninfo>Configure Server: General</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_simond_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
To identify individual users of the system (one simond server can of course serve multiple simon clients), simon and simond use users. Every user has his own speech model. The username / password combination given here is used to log in to simond. If simond does not know the username or the password is incorrect, the connection will fail. See the <ulink url="help:/simond">simond manual</ulink> on how to setup users for simond.</para>

<para>The recognition itself - which is done by the server - might not be available at all times. For example it would not be possible to start the recognition as long as the user does not have a compiled acoustic and language model which has to be created first (during synchronization when all the ingredients - vocabulary, grammar, training - are present). Using the option to start the recognition automatically once it is available, simon will request to start the recognition when it receives the information that it is ready (acoustic and language model is available).</para>

<para>Using the "Connect automatically on simon start" option, simon will automatically start the connection to the configured simond servers after it has finished loading the user interface.</para>
</sect4>

<sect4 id="configure_server_network">
<title>Network</title>
<para>simon connects to simond using TCP/IP.</para>

<para>
<screenshot>
<screeninfo>Configure Server: Network</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_simond_2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
<para>
As of yet (simon 0.3), encryption is not yet supported.
</para>
<para>
The timeout setting specifies, how long simon will wait for a first reply when contacting the hosts. If you are on a very, very slow network and/or use "connect on start" on a very slow machine, you may want to increase this value if you keep getting timeout errors and can resolve them by trying again repeatedly.
</para>
<para>simon supports to be configured to use more than one simond. This is very useful if you for example are going to use simon on a laptop which connects to a different server depending where you are. You could for example add the server you use when you are home and the server used when you are at work. When connecting, simon will try to connect to each of the servers (in order) until it finds one server that accepts the connection.</para>

<para>To add a server, just enter the hostname or IP and the port (separated by ":") or use the dialog that appears when you select the blue arrow next to the input field.</para>

</sect4>
</sect3>

<sect3 id="configure_synchronization">
<title>Synchronization and Model Backup</title>
<para>Here you can configure the model synchronization and restore older versions of your speech model.</para>

<para>
<screenshot>
<screeninfo>Synchronization and Model Backup</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_synchronization.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
Only after the speech model is synchronized the changes take effect and a new restore point is set. This is why per default simon will always synchronize the model with the server when it changes. This is called "Automatic Synchronization" and is the recommended setting. 
</para>
<para>
However, if you want more control you can instruct simon to ask you before starting the synchronization after the model has changed or to rely on manual synchronization all together. When selecting the manual synchronization you have to manually use the "Synchronization" menu item of the simon main window (also see the section <link linkend="main_window">simon main window</link>) every time you want to compile the speech model.
</para>

<para>The simon server will maintain a copy of the last five iterations of model files. However, this only includes the "source files" (the vocabulary, grammar, etc.) - <emphasis>not the compiled model</emphasis>. However, the compiled model will be regenerated from the restored source files automatically.</para>

<para>After you have connected to the server, you can select one of the available models and restore it by clicking on "Restore Model".</para>

<para>Please note that the synchronization will only accept complete source models (containing a vocabulary, a grammar and some trainingssamples) so incomplete models will not be stored on the server and thus not be backed up.</para>

</sect3>
</sect2>



<sect2 id="configure_actions">
<title>Actions</title>

<para>In the actions configuration you can configure the reactions to recognition results.</para>

<sect3 id="configure_actions_recognition">
  <title>Recognition</title>

  <para>The recognition of simon computes not only the most likely result but rather the top ten results.</para>
  <para>Each of the results are assigned a confidence score between 0 and 1 (were 1 is 100% sure).</para>
  <para>Using the <guilabel>Minimum confidence</guilabel> you can set a minimum confidence for recognition results to be considered valid.</para>

  <para>If more than one recognition results are rated higher than the minimum confidence score, simon will provide a popup listing the most likely options for you to choose from.</para>

  <para>
  <screenshot>
  <screeninfo>Did you mean...?</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="did_you_mean.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>This popup can be disabled using the <guibutton>Display selection popup for ambiguous results</guibutton> checkbox.</para>
</sect3>
<sect3>
  <title>Plugin base font</title>
  <para>Many plugins of simon have a graphical user interface.</para>
  <para>The fonts of these interfaces can be configured centrally and independant of the systems font settings here.</para>

  <para>
  <screenshot>
    <screeninfo>Input number with custom font</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="input_number_huge_font.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>
</sect3>

<sect3>
  <title>Lists</title>
  <para>Here you can find the global list element configuration. This serves as a template for new scenarios but is also directly used for <link linkend="configure_actions_recognition">the popup for ambiguous recognition results</link>.</para>
</sect3>
</sect2>

<sect2 id="social_desktop_config">
<title>Social desktop</title>
<para>Scenarios can be <link linkend="export_scenario">uploaded</link> and <link linkend="import_scenario">downloaded from within simon</link>.</para>

<para>For this we use KDEs social desktop facilities and our own <ulink url="http://kde-files.org/index.php?xcontentmode=692">category for simon scenarios on kde-files.org</ulink>.</para>

<para>If you already have an account on <ulink url="http://opendesktop.org">opendesktop.org</ulink> you can input the credentials there. If you don't, you can register directly in the configuration module.</para>

<para>The registration is of course free of charge.</para>
</sect2>



<sect2 id="jconfs">
<title>Adjusting the recognition parameters manually</title>

<para>
simon is targeted towards end-users. It's interface is designed to allow even users without any background in speech technology to design their own language and acoustic models by providing reasonable default values for simple uses.
</para>
<para>
In special cases (severe speech impairments for example), special configuration might be needed. This is why the raw configuration files for the recognition are also respected by simon and can of course be modified to suit your needs.
</para>

<para>There are basically to parts of the Julius configuration that can be adjusted:
<itemizedlist>
<listitem><para>adin.jconf</para><para>This is the configuration of the simon client of the Soundstream sent from simon to the simond. This file is directly read by the adinstreamer.</para>
<para>simon ships with a default adin.jconf without any special parameters. You can change this system wide configuration which will affect all users if there are different user accounts on your machine who all use simon. To just change the configuration of one of those users copy the file to the user path (see below) and edit this copy.</para>
</listitem>
<listitem><para>julius.jconf</para><para>This is a configuration of the simond server and directly influences the recognition. This file is parsed by libjulius and libsent directly.</para>
<para>simond ships with a default julius.jconf. Whenever there is a new user added to the simond database, simond will automatically copy this system wide configuration to the new user. After that the user is of course free to change it but it won't affect the other users. This way the "template" (the system wide configuration) can be changed without affecting other users.</para>
</listitem>
</itemizedlist>
</para>

<para>The path to the Julius configuration files will depend on your platform:

<table frame='all'><title>Julius Configuration Files</title>
<tgroup cols='3' align='left' colsep='1' rowsep='1'>
<colspec colname='c1'/>
<colspec colname='c2'/>
<colspec colname='c3'/>
<thead>
<row>
  <entry>File</entry>
  <entry>Microsoft Windows</entry>
  <entry>GNU/Linux</entry>
</row>
</thead>
<tbody>
<row>
  <entry>adin.jconf (system)</entry>
  <entry>(installation path)\share\apps\simon\adin.jconf</entry>
  <entry>`kde4-config --prefix`/share/apps/simon/adin.jconf</entry>
</row>
<row>
  <entry>adin.jconf (user)</entry>
  <entry>%appdata%\.kde\share\apps\simon\adin.jconf</entry>
  <entry>~/.kde/share/apps/simon/adin.jconf</entry>
</row>
<row>
  <entry>julius.jconf (template)</entry>
  <entry>(installation path)\share\apps\simond\default.jconf</entry>
  <entry>`kde4-config --prefix`/share/apps/simond/default.jconf</entry>
</row>
<row>
  <entry>julius.jconf (user)</entry>
  <entry>%appdata%\.kde\share\apps\simond\models\(user)\active\julius.jconf</entry>
  <entry>~/.kde/share/apps/simond/models/(user)/active/julius.jconf</entry>
</row>
</tbody>
</tgroup>
</table>
</para>

</sect2>

</sect1>

</chapter>



<chapter id="faq">
<title>Questions and Answers</title>

<para>
In an effort to keep this section always up-to-date it is available at our <ulink url="http://www.cyber-byte.at/wiki/index.php/English:_Troubleshooting">online wiki</ulink>.
</para>

</chapter>

<chapter id="credits">

<!-- Include credits for the programmers, documentation writers, and
contributors here. The license for your software should then be included below
the credits with a reference to the appropriate license file included in the KDE
distribution. -->

<title>Credits and License</title>

<para>
&kmyapplication;
</para>
<para>
Program copyright 2006-2009 Peter Grasch <email>grasch@simon-listens.org</email>, Phillip Goriup, Tschernegg Susanne, Bettina Sturmann, Martin Gigerl
</para>

<para>
Documentation Copyright &copy; 2009 Peter Grasch <email>grasch@simon-listens.org</email>
</para>

<!-- TRANS:CREDIT_FOR_TRANSLATORS -->

&underFDL;               <!-- FDL: do not remove -->

<!-- Determine which license your application is licensed under,
     and delete all the remaining licenses below:

     (NOTE:  All documentation are licensed under the FDL,
     regardless of what license the application uses) -->

&underGPL;        	 <!-- GPL License -->

</chapter>

<appendix id="installation">
<title>Installation</title>
<para>Please see our <ulink url="http://www.cyber-byte.at/wiki/index.php/English:_Setup">wiki</ulink> for install instructions.</para>

</appendix>

&documentation.index;
</book>

<!--
Local Variables:
mode: xml
sgml-minimize-attributes:nil
sgml-general-insert-case:lower
sgml-indent-step:0
sgml-indent-data:nil
End:

vim:tabstop=2:shiftwidth=2:expandtab
kate: space-indent on; indent-width 2; tab-width 2; indent-mode none;
-->
